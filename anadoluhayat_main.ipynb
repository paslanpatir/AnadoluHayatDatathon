{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The aim is to predict ARTIS_DURUMU, which a categorical variable with levels 0 and 1.\n\nTarget is imbalanced, so\n\n* weights can be introduced\n* a proper cutoff can be selected\n* downsampling or subsampling can be applied","metadata":{}},{"cell_type":"markdown","source":"# Load Libraries & Read Data","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n        import matplotlib.pyplot as plt\nfrom geopy import distance\nimport seaborn as sns\n%matplotlib inline\nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom sklearn import neighbors\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nimport json\nimport gc\npd.options.display.max_rows=300","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-23T20:31:44.790199Z","iopub.execute_input":"2022-05-23T20:31:44.790559Z","iopub.status.idle":"2022-05-23T20:31:48.174869Z","shell.execute_reply.started":"2022-05-23T20:31:44.790469Z","shell.execute_reply":"2022-05-23T20:31:48.173505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv(\"../input/anadolu-hayat-emeklilik-datathon-coderspace/samplesubmission.csv\")\ntest = pd.read_csv(\"../input/anadolu-hayat-emeklilik-datathon-coderspace/test.csv\")\ntrain = pd.read_csv(\"../input/anadolu-hayat-emeklilik-datathon-coderspace/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-13T01:37:15.563745Z","iopub.execute_input":"2022-05-13T01:37:15.564303Z","iopub.status.idle":"2022-05-13T01:37:22.351097Z","shell.execute_reply.started":"2022-05-13T01:37:15.564244Z","shell.execute_reply":"2022-05-13T01:37:22.350155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine data sets for convenience in feature engineering\ndt = pd.concat([train, test])\n# Change column names for convenience\ncols = dt.columns\ncols =list(map(str.lower, cols))\n\ndt.columns = cols\ndt.shape\ndt.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Data _ preliminaries\n\n* Check target distribution\n* Check duplicates\n* Check missing values\n* Check unique values of categorical columns","metadata":{}},{"cell_type":"code","source":"### check target distribution\ndt['artis_durumu'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### check if there are duplicates\nprint(dt.shape[0])\n# counting unique values\nn = len(pd.unique(dt['policy_id']))\nprint(n)\n\n# data is not duplicated , good","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:14:25.774751Z","iopub.execute_input":"2022-05-12T08:14:25.775269Z","iopub.status.idle":"2022-05-12T08:14:25.844147Z","shell.execute_reply.started":"2022-05-12T08:14:25.775236Z","shell.execute_reply":"2022-05-12T08:14:25.843281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Check missing values\n#if they are all missing in either training or test, or check if complete columns in training set have missing values in test set\n\n# Same columns have missing values in both training and test sets\ntrain.isna().any()[lambda x: x]\ntest.isna().any()[lambda x: x]","metadata":{"execution":{"iopub.status.busy":"2022-05-12T06:23:52.791825Z","iopub.execute_input":"2022-05-12T06:23:52.792105Z","iopub.status.idle":"2022-05-12T06:23:54.055709Z","shell.execute_reply.started":"2022-05-12T06:23:52.792071Z","shell.execute_reply":"2022-05-12T06:23:54.054675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Check unique values of string cols\n# string cols\nstring_cols = dt.loc[:, dt.dtypes == object].columns\nstring_cols\n\nfor ix in string_cols:\n    print(ix)\n    print(pd.unique(dt[ix]))\n    print(dt[ix].value_counts())\n    print(\"-------------\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-12T08:14:30.71962Z","iopub.execute_input":"2022-05-12T08:14:30.719966Z","iopub.status.idle":"2022-05-12T08:14:34.574384Z","shell.execute_reply.started":"2022-05-12T08:14:30.719933Z","shell.execute_reply":"2022-05-12T08:14:34.573282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Suggestions for categorical features**\n* sozlesme_kokeni: 'NEW' dominates\n    * sozlesme_kokeni_NEW: boolean\n    * frequency encoding\n    \n    \n* sozlesme_kokeni_detay: 'NEW' dominates\n    * sozlesme_kokeni_detay_NEW: boolean\n    * frequency encoding\n\n\n* baslangic_tarihi : look for distributions over years and months (obtain year-month col)\n\n* kapsam_tipi: group low density pensions types / also check for its numeric\n    * frequency encoding\n    \n    \n* kapsam_grubu : gruplanmamis dominates\n    * \n    \n    \n* dagitim_kanali : group most frequent ones\n    * \n    \n    \n* police_sehir: group most frequent ones\n    * \n    \n    \n* uyruk:\n    * uyruk_TR:boolean\n    \n    \n* meslek & meslek_kirilim\n    * \n    \n    \n* yatirim karakteri: nice to have <3, but lots of na\n    * one-hot encoding\n    \n    \n* medeni_hal: group single vs married / or also add widowed\n    * one-hot encoding\n    \n    \n* egitim durumu: ok , but maybe group higher education\n    * ordinal encoding\n","metadata":{}},{"cell_type":"markdown","source":"# Playing with Columns","metadata":{}},{"cell_type":"code","source":"# clean 'gelir' columns\nchar_gelir = dt['gelir'].str.contains(\",\", regex=False, na=False)\ndt['gelir'][char_gelir] = dt['gelir'][char_gelir].str.replace('\\W', '', )\ndt['gelir'][char_gelir] = dt['gelir'][char_gelir] *100\n\ndt['gelir'] = dt['gelir'].astype(float)\n\ndt[\"gelir\"].describe([0.0001, 0.0005, 0.01, 0.05, 0.20, 0.25,0.4,0.5,0.6,0.75, 0.8, 0.95, 0.96, 0.97, 0.98, 0.99, 0.9995, 0.9999]).astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:14:34.832654Z","iopub.execute_input":"2022-05-12T08:14:34.832997Z","iopub.status.idle":"2022-05-12T08:14:36.497279Z","shell.execute_reply.started":"2022-05-12T08:14:34.832963Z","shell.execute_reply":"2022-05-12T08:14:36.496192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inf values\n#dt['gelir'].idxmax()\n#dt.iloc[323636]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.loc[dt['gelir'] < 0, 'gelir'] = 0 # nonnegative\ndt.loc[dt['gelir'] > 21000, 'gelir'] = 21000 # upper cap\ndt.loc[dt['gelir'] < 100,'gelir'] = dt.loc[dt['gelir'] < 100]['gelir']* 10 # probably wrong records\n\ndt[\"gelir\"].describe([0.0001, 0.0005, 0.01, 0.05, 0.20, 0.25,0.4,0.5,0.6,0.75, 0.8, 0.95, 0.96, 0.97, 0.98, 0.99, 0.9995, 0.9999]).astype(float)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:14:42.191046Z","iopub.execute_input":"2022-05-12T08:14:42.192128Z","iopub.status.idle":"2022-05-12T08:14:42.267892Z","shell.execute_reply.started":"2022-05-12T08:14:42.192069Z","shell.execute_reply":"2022-05-12T08:14:42.266879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cocuk_sayisi\n#plt.hist(dt[\"cocuk_sayisi\"])\ndt.loc[dt['cocuk_sayisi'] >5, 'cocuk_sayisi'] = 5","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:11:18.051568Z","iopub.execute_input":"2022-05-12T10:11:18.052008Z","iopub.status.idle":"2022-05-12T10:11:18.065791Z","shell.execute_reply.started":"2022-05-12T10:11:18.051966Z","shell.execute_reply":"2022-05-12T10:11:18.064668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# baslangic tarihi related\ndt[\"year\"] = pd.to_datetime(dt[\"baslangic_tarihi\"]).dt.year\ndt[\"month\"] = pd.to_datetime(dt[\"baslangic_tarihi\"]).dt.month\n\ndt['yearmonth'] = dt['baslangic_tarihi'].str.replace('\\W', '', )\ndt['yearmonth'] = dt['yearmonth'].astype('int')","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:14:46.399991Z","iopub.execute_input":"2022-05-12T08:14:46.400274Z","iopub.status.idle":"2022-05-12T08:14:48.417545Z","shell.execute_reply.started":"2022-05-12T08:14:46.400245Z","shell.execute_reply":"2022-05-12T08:14:48.416588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# month\ndt.loc[dt['month']==1, 'month'] = 13 # month 1 & 12 are popular months","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.rename(columns = {'subat_odenen_tu':'subat_odenen_tutar'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:14:50.861424Z","iopub.execute_input":"2022-05-12T08:14:50.861803Z","iopub.status.idle":"2022-05-12T08:14:50.868743Z","shell.execute_reply.started":"2022-05-12T08:14:50.861769Z","shell.execute_reply":"2022-05-12T08:14:50.867518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define columns\nmonths = [\"ocak\",\"subat\",\"mart\",\"nisan\",\"mayis\",\"haziran\",\"temmuz\",\"agustos\",\"eylul\",\"ekim\",\"kasim\",\"aralik\"]\nvade_cols = [str(i)+'_vade_tutari' for i in months]\nodenen_cols = [str(i)+'_odenen_tutar' for i in months]\n\nfor ix in months:\n    dt[ix + '_odeme_orani']  = 1.00*dt[ix + '_odenen_tutar']/ dt[ix + '_vade_tutari'] \n    \ndt['average_vade_tutari'] = dt[vade_cols].mean(axis=1)\ndt['min_vade_tutari'] = dt[vade_cols].min(axis=1)\ndt['max_vade_tutari'] = dt[vade_cols].max(axis=1)\n\ndt['change_in_vade_tutari'] = 1.00*(dt['max_vade_tutari'] - dt['min_vade_tutari'])/dt['min_vade_tutari']\n\ndt['vade_vs_gelir'] = 1.00*dt['gelir']/dt['average_vade_tutari']\ndt['vade_vs_gelir'].fillna(dt['vade_vs_gelir'].mean(), inplace = True)\n\nfor ix in months:\n    ispaid =np.where(((dt[ix + '_vade_tutari']*0.01 +dt[ix + '_odenen_tutar']) - dt[ix + '_vade_tutari']) >0, 1, 0)\n    dt[ix + '_ispaid'] = ispaid\n    \n# number of not paid\nispaid_cols = [col for col in dt.columns if 'ispaid' in col]\ndt['toplam_notpaid'] = 12 - dt[ispaid_cols].sum(axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## PART 2 _ If you are bothered with outliers, then \n# vade cols duzeltme_1\n# correct with very large values: ex : policy_id 7981587\n# correct ex :5177519\ndt[vade_cols] = np.where(dt[vade_cols] > 100000, dt[vade_cols]/1000, dt[vade_cols])\ndt[odenen_cols] = np.where(dt[odenen_cols] > 100000, dt[odenen_cols]/1000, dt[odenen_cols])\n\n# IQR Outlier Tending for odenen tutar columns\nQ1 = dt[odenen_cols].T.quantile(0.25)#lower = (mn - 3*sd)\nQ3 = dt[odenen_cols].T.quantile(0.75)#upper = (mn + 3*sd)\nIQR = Q3-Q1\n\nlower = np.where((Q1 - 1.5*IQR)<0,0,Q1 - 1.5*IQR)\nupper = Q3 + 1.5*IQR\n\nfor j in odenen_cols:\n    dt[j] = np.where(dt[j]< lower, lower,dt[j])\n    dt[j] = np.where(dt[j]> upper, upper,dt[j])\n    \n# odenen correction after things\n# cannot pay more than vade\nfor j in range(12):\n    dt[odenen_cols[j]] = np.where(dt[odenen_cols[j]]>(1.1*dt[vade_cols[j]]),dt[vade_cols[j]],dt[odenen_cols[j]])\n\n#update statistical columns\ndt['average_vade_tutari'] = dt[vade_cols].mean(axis=1)\ndt['min_vade_tutari'] = dt[vade_cols].min(axis=1)\ndt['max_vade_tutari'] = dt[vade_cols].max(axis=1)\n\ndt['change_in_vade_tutari'] = 1.00*(dt['max_vade_tutari'] - dt['min_vade_tutari'])/dt['min_vade_tutari']\n\ndt['vade_vs_gelir'] = 1.00*dt['gelir']/dt['average_vade_tutari']\ndt['vade_vs_gelir'].fillna(dt['vade_vs_gelir'].mean(), inplace = True)\n\n# add new columns\ndt['total_vade_tutari'] = dt[vade_cols].sum(axis=1)\ndt['sd_vade_tutari']= dt[vade_cols].std(axis=1)\ndt['total_odenen_tutar'] = dt[odenen_cols].sum(axis=1)\ndt['sd_odenen_tutar']= dt[odenen_cols].std(axis=1)\n#dt[dt['total_odenen_tutar']>dt['total_vade_tutari']]\n#dt[dt['total_odenen_tutar']== 0]['artis_durumu'].value_counts()\n\ndt['odeme_ratio'] = 1.00*dt['total_odenen_tutar']/dt['total_vade_tutari']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## General behavior of the policy owner\nk = vade_cols.copy()\nk.extend(odenen_cols)\nk.extend(['policy_id'])\nsub_dt = dt[k].copy()\n\n#initialize\nPD_OMEDI=[0] * sub_dt.shape[0]\nPD_ODEDI=[0] * sub_dt.shape[0]\nPY_OMEDI=[0] * sub_dt.shape[0]\nPY_ODEDI=[0] * sub_dt.shape[0]\nPA_OMEDI=[0] * sub_dt.shape[0]\nPA_ODEDI=[0] * sub_dt.shape[0]\n    \nfor j in range(1,12):\n    vade = np.where(sub_dt[vade_cols[j]] - sub_dt[vade_cols[j-1]] <0,-1,np.where(sub_dt[vade_cols[j]] - sub_dt[vade_cols[j-1]] >0,1,0))\n    odeme = np.where(sub_dt[vade_cols[j]] - (1.01*sub_dt[odenen_cols[j]]) >0,-1,1)\n    \n    PD_OMEDI = PD_OMEDI+np.where((vade == -1) & (odeme == -1),1,0)\n    PD_ODEDI = PD_ODEDI+np.where((vade == -1) & (odeme == 1),1,0)\n    \n    PY_OMEDI = PY_OMEDI+np.where((vade == 1) & (odeme == -1),1,0)\n    PY_ODEDI = PY_ODEDI+np.where((vade == 1) & (odeme == 1),1,0)  \n    \n    PA_OMEDI = PA_OMEDI+np.where((vade == 0) & (odeme == -1),1,0)\n    PA_ODEDI = PA_ODEDI+np.where((vade == 0) & (odeme == 1),1,0)  \n         \n                                          \nsub_dt['PD_OMEDI']=PD_OMEDI      \nsub_dt['PD_ODEDI']=PD_ODEDI  \nsub_dt['PY_OMEDI']=PY_OMEDI  \nsub_dt['PY_ODEDI']=PY_ODEDI  \nsub_dt['PA_OMEDI']=PA_OMEDI  \nsub_dt['PA_ODEDI']=PA_ODEDI  \n\nsub_dt = sub_dt[['policy_id','PD_OMEDI','PD_ODEDI','PY_OMEDI','PY_ODEDI','PA_OMEDI','PA_ODEDI']]\ndt= dt.merge(sub_dt, on = \"policy_id\", how= 'left')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt['kapsam_tipi_numeric'] = dt['kapsam_tipi'].str.replace('PENSION', '', )\ndt['kapsam_tipi_numeric'] = dt['kapsam_tipi_numeric'].astype('int')\n\ndt['hesap_degeri_degisimi'] = dt['sene_sonu_hesap_degeri'] - dt['sene_basi_hesap_degeri']\ndt['hesap_degeri_degisimi_perc'] = (dt['sene_sonu_hesap_degeri'] - dt['sene_basi_hesap_degeri']) / dt['sene_basi_hesap_degeri']\n\ndt['hesap_degeri_degisimi_perc'].fillna(0, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:15:01.570411Z","iopub.execute_input":"2022-05-12T08:15:01.570706Z","iopub.status.idle":"2022-05-12T08:15:02.640635Z","shell.execute_reply.started":"2022-05-12T08:15:01.570674Z","shell.execute_reply":"2022-05-12T08:15:02.639654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Represent the trend in odenen tutar and vade tutari over months**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\ndef find_trend(ys):\n    xs = range(12)\n    xs = np.array(xs).reshape(-1,1)\n    #ys = dt.loc[3313][vade_cols]\n    ys = np.array(ys)\n    model = LinearRegression(fit_intercept=True)\n    model.fit(xs, ys)\n    # calculate trend\n    trend = model.coef_\n    return trend[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#1\ndt['trend_vade']= dt.apply(lambda x: find_trend(x[vade_cols]), axis=1)\n\n#2 ( diff between odenen and vade tutari)\nnew_cols = [str(i)+'_vade_odenen_fark' for i in months]\nfor j in range(12):\n    dt.loc[:,str(new_cols[j])] = dt[vade_cols[j]]-dt[odenen_cols[j]]\n    \ndt['trend_vade_odenen_fark']= dt.apply(lambda x: find_trend(x[new_cols]), axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# office puan\noffice_dt= dt.groupby('office_id')[vade_cols].mean().reset_index()\noffice_dt['office_mean_vade_tutari']= office_dt.mean(axis=1)\noffice_dt = office_dt[[\"office_id\",'office_mean_vade_tutari']].copy()\ndt= dt.merge(office_dt, on = \"office_id\", how= 'left')\ndt['vade_in_office']= dt['average_vade_tutari']/dt['office_mean_vade_tutari']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dt.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-11T07:47:00.711317Z","iopub.execute_input":"2022-05-11T07:47:00.71246Z","iopub.status.idle":"2022-05-11T07:47:02.020656Z","shell.execute_reply.started":"2022-05-11T07:47:00.712397Z","shell.execute_reply":"2022-05-11T07:47:02.019449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imputation & Encoding","metadata":{}},{"cell_type":"code","source":"# NA columns\ndt.isna().any()[lambda x: x]\n#dt.head().T","metadata":{"execution":{"iopub.status.busy":"2022-05-12T06:29:01.715792Z","iopub.execute_input":"2022-05-12T06:29:01.716092Z","iopub.status.idle":"2022-05-12T06:29:02.932963Z","shell.execute_reply.started":"2022-05-12T06:29:01.716057Z","shell.execute_reply":"2022-05-12T06:29:02.932288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dt[(dt['sozlesme_kokeni'] == \"NEW\") & (dt['sozlesme_kokeni_detay'].isnull == True)]\n## empty, then \ndt['sozlesme_kokeni_detay'].fillna(\"other\", inplace = True)\n\n## only a few missing- dominant category\ndt['dagitim_kanali'].fillna(\"Kanal4 + Kanal2\", inplace = True)\ndt['uyruk'].fillna(\"TR\", inplace = True)\ndt['musteri_segmenti'].fillna(106, inplace = True)\n\n## highly missing\ndt['yatirim_karakteri'].fillna(\"other\", inplace = True)\n\n## median impute\ndt['cocuk_sayisi'].fillna(dt['cocuk_sayisi'].median(), inplace = True)\n\n## reasoning\n#1\noffice_vs_sehir = dt.groupby(\"office_id\")[\"police_sehir\"].agg(lambda x:x.value_counts().index[0]).reset_index()\noffice_vs_sehir = office_vs_sehir.rename(columns={\"police_sehir\":\"most_common_sehir\"})\noffice_vs_sehir.head()\n\ndt = dt.merge(office_vs_sehir, how=\"left\",on=\"office_id\")\ndt['police_sehir'].fillna(dt['most_common_sehir'], inplace = True)\n\nmost_common_sehir =  dt[\"police_sehir\"].value_counts().index[0]\ndt['police_sehir'].fillna(most_common_sehir, inplace = True)\n\n#2\nmost_common_meslek =  dt[\"meslek\"].value_counts().index[0]\ndt['meslek'].fillna(most_common_meslek, inplace = True)\n\n#3\nmeslek_vs_kirilim = dt.groupby(\"meslek\")[\"meslek_kirilim\"].agg(lambda x:x.value_counts().index[0]).reset_index()\nmeslek_vs_kirilim = meslek_vs_kirilim.rename(columns={\"meslek_kirilim\":\"most_common_kirilim\"})\nmeslek_vs_kirilim.head()\n\ndt = dt.merge(meslek_vs_kirilim, how=\"left\",on=\"meslek\")\ndt['meslek_kirilim'].fillna(dt['most_common_kirilim'], inplace = True)\n\nmost_common_kirilim =  dt[\"meslek_kirilim\"].value_counts().index[0]\ndt['meslek_kirilim'].fillna(most_common_kirilim, inplace = True)\n\n## combining levels\n#1\npd.crosstab(dt['medeni_hal'],dt['artis_durumu'])\n\nmedeni_hal_upd = {\"Divorced\": \"2\", \n                  \"Marriage Cancelled\": \"0\",\n                  \"Married\": \"1\",\n                  \"Other\": np.NaN,\n                  \"Single\":\"0\",\n                  \"Widowed\":\"2\"}\ndt['medeni_hal'].replace(medeni_hal_upd, inplace=True)\ndt['medeni_hal'] = dt['medeni_hal'].astype('float')\n\n#2\npd.crosstab(dt['egitim_durum'],dt['artis_durumu'])\negitim_durum_upd = {\"(Di?er)\": \"0\",\n                    \"?lkö?retim\": \"1\",\n                    \"Lise\": \"2\",\n                    \"Önlisans\": \"3\",\n                    \"Lisans\": \"4\",\n                    \"Yüksek Lisans\": \"5\",\n                    \"Doktora\":\"6\"}\ndt['egitim_durum'].replace(egitim_durum_upd, inplace=True)\ndt['egitim_durum'] = dt['egitim_durum'].astype('float')","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:15:07.802566Z","iopub.execute_input":"2022-05-12T08:15:07.803749Z","iopub.status.idle":"2022-05-12T08:15:17.79457Z","shell.execute_reply.started":"2022-05-12T08:15:07.803688Z","shell.execute_reply":"2022-05-12T08:15:17.793493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from fancyimpute import MICE #MICE()  depreciated\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nselected_columns_for_imputation = ['year','dogum_tarihi','cinsiyet','memleket','medeni_hal','musteri_segmenti','egitim_durum','gelir','cocuk_sayisi','sene_basi_hesap_degeri','sene_sonu_hesap_degeri']\nMiceImputed = dt[selected_columns_for_imputation].copy(deep=True)\nmice_imputer = IterativeImputer()\nMiceImputed.iloc[:, :]= mice_imputer.fit_transform(MiceImputed)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:15:34.243601Z","iopub.execute_input":"2022-05-12T08:15:34.2442Z","iopub.status.idle":"2022-05-12T08:15:54.617927Z","shell.execute_reply.started":"2022-05-12T08:15:34.244163Z","shell.execute_reply":"2022-05-12T08:15:54.616581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MiceImputed.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T06:30:41.574947Z","iopub.execute_input":"2022-05-12T06:30:41.575665Z","iopub.status.idle":"2022-05-12T06:30:41.595118Z","shell.execute_reply.started":"2022-05-12T06:30:41.575621Z","shell.execute_reply":"2022-05-12T06:30:41.594238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NA columns\n#dt.isna().any()[lambda x: x]\ndt['medeni_hal'].fillna(MiceImputed['medeni_hal'], inplace = True)\ndt['egitim_durum'].fillna(MiceImputed['egitim_durum'], inplace = True)\ndt['gelir'].fillna(MiceImputed['gelir'], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:15:58.379401Z","iopub.execute_input":"2022-05-12T08:15:58.37974Z","iopub.status.idle":"2022-05-12T08:15:58.400873Z","shell.execute_reply.started":"2022-05-12T08:15:58.379708Z","shell.execute_reply":"2022-05-12T08:15:58.399847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Other Encoding","metadata":{}},{"cell_type":"markdown","source":"### Boolean Encoding","metadata":{}},{"cell_type":"code","source":"#1\ndt.loc[dt['sozlesme_kokeni'] == \"NEW\", 'sozlesme_kokeni_NEW'] = 1\ndt['sozlesme_kokeni_NEW'] = dt['sozlesme_kokeni_NEW'].fillna(0)\n\npd.crosstab(dt['sozlesme_kokeni_NEW'],dt['artis_durumu'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#2\ndt.loc[dt['sozlesme_kokeni_detay'] == \"NEW\", 'sozlesme_kokeni_detay_NEW'] = 1\ndt['sozlesme_kokeni_detay_NEW'] = dt['sozlesme_kokeni_detay_NEW'].fillna(0)\n\npd.crosstab(dt['sozlesme_kokeni_detay_NEW'],dt['artis_durumu'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#3\ndt.loc[dt['uyruk'] == \"TR\", 'uyruk_TR'] = 1\ndt['uyruk_TR'] = dt['uyruk_TR'].fillna(0)\n\npd.crosstab(dt['uyruk_TR'],dt['artis_durumu'])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:16:01.124465Z","iopub.execute_input":"2022-05-12T08:16:01.124804Z","iopub.status.idle":"2022-05-12T08:16:01.393944Z","shell.execute_reply.started":"2022-05-12T08:16:01.124773Z","shell.execute_reply":"2022-05-12T08:16:01.39285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.pivot(columns='artis_durumu').year.plot(kind = 'hist', stacked=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:05:03.880829Z","iopub.execute_input":"2022-05-11T09:05:03.881813Z","iopub.status.idle":"2022-05-11T09:05:15.327722Z","shell.execute_reply.started":"2022-05-11T09:05:03.881761Z","shell.execute_reply":"2022-05-11T09:05:15.326826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.pivot(columns='artis_durumu').month.plot(kind = 'hist', stacked=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:06:23.772909Z","iopub.execute_input":"2022-05-11T09:06:23.773496Z","iopub.status.idle":"2022-05-11T09:06:34.815712Z","shell.execute_reply.started":"2022-05-11T09:06:23.773442Z","shell.execute_reply":"2022-05-11T09:06:34.815102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"egitim_durum_upd","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:13:18.003794Z","iopub.execute_input":"2022-05-11T09:13:18.004509Z","iopub.status.idle":"2022-05-11T09:13:18.01161Z","shell.execute_reply.started":"2022-05-11T09:13:18.004467Z","shell.execute_reply":"2022-05-11T09:13:18.010474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### one-hot encodings\n\n* medeni_hal\n* yatirim_karakteri (without other)","metadata":{}},{"cell_type":"code","source":"dt['medeni_hal'] = dt['medeni_hal'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:16:13.993147Z","iopub.execute_input":"2022-05-12T08:16:13.993526Z","iopub.status.idle":"2022-05-12T08:16:14.14544Z","shell.execute_reply.started":"2022-05-12T08:16:13.993491Z","shell.execute_reply":"2022-05-12T08:16:14.1445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nonehot_cols = ['medeni_hal', 'yatirim_karakteri']\n\nenc = OneHotEncoder()\n# transforming the column after fitting\nenc.fit(dt[onehot_cols])\n\nenc.get_feature_names_out(['medeni_hal', 'yatirim_karakteri'])\n\nencoded_colm = enc.transform(dt[onehot_cols]).toarray()\nencoded_colm = pd.DataFrame(encoded_colm, columns = enc.get_feature_names_out(['medeni_hal', 'yatirim_karakteri']))\nencoded_colm\n\n# concatenating dataframes\ndt = pd.concat([dt, encoded_colm], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:16:17.653794Z","iopub.execute_input":"2022-05-12T08:16:17.654163Z","iopub.status.idle":"2022-05-12T08:16:19.488845Z","shell.execute_reply.started":"2022-05-12T08:16:17.654126Z","shell.execute_reply":"2022-05-12T08:16:19.487804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop columns\n#dt.drop(columns=['medeni_hal_0', 'medeni_hal_1'] , inplace =True)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T09:43:25.306386Z","iopub.execute_input":"2022-05-11T09:43:25.306712Z","iopub.status.idle":"2022-05-11T09:43:25.670696Z","shell.execute_reply.started":"2022-05-11T09:43:25.306675Z","shell.execute_reply":"2022-05-11T09:43:25.669973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hash encodings with high cardinality\n\nhttps://towardsdatascience.com/smarter-ways-to-encode-categorical-data-for-machine-learning-part-1-of-3-6dca2f71b159","metadata":{}},{"cell_type":"code","source":"import category_encoders as ce","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:17:44.641189Z","iopub.execute_input":"2022-05-12T08:17:44.64159Z","iopub.status.idle":"2022-05-12T08:17:44.645951Z","shell.execute_reply.started":"2022-05-12T08:17:44.641551Z","shell.execute_reply":"2022-05-12T08:17:44.645115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kapsam_tipi \nce_hash = ce.HashingEncoder(cols = ['kapsam_tipi'],n_components=12)\nce_hash.fit(dt)\n\nhashing_output = ce_hash.transform(dt)\noutput_cols = [col for col in hashing_output.columns if 'col_' in col]\nhashing_output = hashing_output[output_cols]\n\nhash_cols= [\"Kapsam_Tipi_\" + str(i) for i in range(1,13)]\nhashing_output.columns = hash_cols\nhashing_output.head()\n\ndt = pd.concat([dt, hashing_output], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:37:52.043789Z","iopub.execute_input":"2022-05-12T08:37:52.044232Z","iopub.status.idle":"2022-05-12T08:43:26.446161Z","shell.execute_reply.started":"2022-05-12T08:37:52.044192Z","shell.execute_reply":"2022-05-12T08:43:26.444984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# police_sehir \nce_hash = ce.HashingEncoder(cols = ['police_sehir'],n_components=12)\nce_hash.fit(dt)\n\nhashing_output = ce_hash.transform(dt)\noutput_cols = [col for col in hashing_output.columns if 'col_' in col]\nhashing_output = hashing_output[output_cols]\n\nhash_cols= [\"police_sehir_\" + str(i) for i in range(1,13)]\nhashing_output.columns = hash_cols\nhashing_output.head()\n\ndt = pd.concat([dt, hashing_output], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:30:44.610814Z","iopub.execute_input":"2022-05-12T08:30:44.611186Z","iopub.status.idle":"2022-05-12T08:36:00.979775Z","shell.execute_reply.started":"2022-05-12T08:30:44.611149Z","shell.execute_reply":"2022-05-12T08:36:00.97871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clustering for Meslek","metadata":{}},{"cell_type":"code","source":"meslek_based = dt.groupby(\"meslek\").agg({\n    \"gelir\":\"mean\",\n    \"sene_basi_hesap_degeri\": \"mean\",\n    \"sene_sonu_hesap_degeri\": \"mean\",\n    \"average_vade_tutari\" : \"mean\",\n    \"max_vade_tutari\" :\"mean\",\n    \"policy_id\":\"count\"\n}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:59:34.876802Z","iopub.execute_input":"2022-05-12T09:59:34.877169Z","iopub.status.idle":"2022-05-12T09:59:34.999529Z","shell.execute_reply.started":"2022-05-12T09:59:34.877136Z","shell.execute_reply":"2022-05-12T09:59:34.998193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#meslek_based['policy_id'].describe()\nmeslek_based.head()\n#meslek_based = meslek_based[meslek_based[\"policy_id\"] > meslek_based[\"policy_id\"].quantile(0)].sort_values(by=\"gelir\", ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:59:46.327354Z","iopub.execute_input":"2022-05-12T09:59:46.327683Z","iopub.status.idle":"2022-05-12T09:59:46.342816Z","shell.execute_reply.started":"2022-05-12T09:59:46.327649Z","shell.execute_reply":"2022-05-12T09:59:46.341894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score","metadata":{"execution":{"iopub.status.busy":"2022-05-17T19:33:14.821033Z","iopub.execute_input":"2022-05-17T19:33:14.821318Z","iopub.status.idle":"2022-05-17T19:33:14.904487Z","shell.execute_reply.started":"2022-05-17T19:33:14.821288Z","shell.execute_reply":"2022-05-17T19:33:14.903905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = meslek_based.loc[:, ~meslek_based.columns.isin(['meslek','policy_id'])]\nscaler = preprocessing.StandardScaler().fit(X)\nX_scaled = scaler.transform(X)\n\nkmeans_kwargs = {\n    \"init\": \"random\",\n    \"n_init\": 10,\n    \"max_iter\": 300,\n    \"random_state\": 42,\n}\n# A list holds the SSE values for each k\nsse = []\nfor k in range(1, 21):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(X_scaled)\n    sse.append(kmeans.inertia_)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:59:50.224565Z","iopub.execute_input":"2022-05-12T09:59:50.22533Z","iopub.status.idle":"2022-05-12T09:59:50.504054Z","shell.execute_reply.started":"2022-05-12T09:59:50.225288Z","shell.execute_reply":"2022-05-12T09:59:50.503227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"fivethirtyeight\")\nplt.plot(range(1, 21), sse)\nplt.xticks(range(1, 21))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:59:52.529945Z","iopub.execute_input":"2022-05-12T09:59:52.530316Z","iopub.status.idle":"2022-05-12T09:59:52.780205Z","shell.execute_reply.started":"2022-05-12T09:59:52.53028Z","shell.execute_reply":"2022-05-12T09:59:52.779288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(\n    init=\"random\",\n    n_clusters=7,\n    n_init=10,\n    max_iter=300,\n    random_state=42\n).fit(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T09:59:58.539474Z","iopub.execute_input":"2022-05-12T09:59:58.540184Z","iopub.status.idle":"2022-05-12T09:59:58.57009Z","shell.execute_reply.started":"2022-05-12T09:59:58.54014Z","shell.execute_reply":"2022-05-12T09:59:58.567195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cluster_summary\npd.concat([pd.DataFrame(kmeans.labels_, columns = [\"cluster\"]),meslek_based[\"policy_id\"]], axis=1).groupby(\"cluster\").agg({\n    \"cluster\": \"count\",\n    \"policy_id\":\"sum\"\n})","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:00:07.468025Z","iopub.execute_input":"2022-05-12T10:00:07.468823Z","iopub.status.idle":"2022-05-12T10:00:07.484881Z","shell.execute_reply.started":"2022-05-12T10:00:07.468766Z","shell.execute_reply":"2022-05-12T10:00:07.483524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cluster means\nmeslek_clusters = pd.DataFrame(kmeans.cluster_centers_, columns = X.columns)\nmeslek_clusters.sort_values(\"gelir\",ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:00:00.453155Z","iopub.execute_input":"2022-05-12T10:00:00.453486Z","iopub.status.idle":"2022-05-12T10:00:00.469992Z","shell.execute_reply.started":"2022-05-12T10:00:00.453438Z","shell.execute_reply":"2022-05-12T10:00:00.468883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meslek_clusters = pd.concat([meslek_based[\"meslek\"], pd.DataFrame(kmeans.labels_, columns = [\"meslek_cluster\"])], axis = 1)\ndt = dt.merge(meslek_clusters, on = \"meslek\", how=\"left\")","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:00:14.699064Z","iopub.execute_input":"2022-05-12T10:00:14.699425Z","iopub.status.idle":"2022-05-12T10:00:14.706334Z","shell.execute_reply.started":"2022-05-12T10:00:14.69939Z","shell.execute_reply":"2022-05-12T10:00:14.705394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#meslek_clusters.sort_values(\"meslek_cluster\")","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:01:37.458529Z","iopub.execute_input":"2022-05-12T10:01:37.458899Z","iopub.status.idle":"2022-05-12T10:01:37.463959Z","shell.execute_reply.started":"2022-05-12T10:01:37.458859Z","shell.execute_reply":"2022-05-12T10:01:37.462799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### frequency encodings","metadata":{}},{"cell_type":"code","source":"# grouping by frequency\nfq = dt.groupby('meslek_kirilim').size()/len(dt)  \n# mapping values to dataframe\ndt.loc[:, \"{}_encoded\".format('meslek_kirilim')] = dt['meslek_kirilim'].map(fq) \n## drop original column.\n#df = df.drop(['nom_0'], axis = 1)\n#fq.plot.bar(stacked = True) \n#df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:05:21.992696Z","iopub.execute_input":"2022-05-12T10:05:21.993094Z","iopub.status.idle":"2022-05-12T10:05:22.208781Z","shell.execute_reply.started":"2022-05-12T10:05:21.993059Z","shell.execute_reply":"2022-05-12T10:05:22.207409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### target encodings","metadata":{}},{"cell_type":"code","source":"from category_encoders import TargetEncoder","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:06:46.325867Z","iopub.execute_input":"2022-05-12T10:06:46.326512Z","iopub.status.idle":"2022-05-12T10:06:46.333201Z","shell.execute_reply.started":"2022-05-12T10:06:46.326438Z","shell.execute_reply":"2022-05-12T10:06:46.331037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dagitim_tipi\ntarget_X = dt[dt['artis_durumu'].isnull() == False]['dagitim_kanali']\ntarget_y = dt[dt['artis_durumu'].isnull() == False]['artis_durumu']\nce_target = TargetEncoder(cols=['dagitim_kanali'], smoothing=8, min_samples_leaf=5).fit(target_X,target_y)\n\nce_output = ce_target.transform(dt['dagitim_kanali'].reset_index(drop=True))\ndt['dagitim_kanali_encoded'] = ce_output['dagitim_kanali']\n\n#ce_output.value_counts()\npd.crosstab(dt['kapsam_grubu'],dt['artis_durumu'])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:06:48.168615Z","iopub.execute_input":"2022-05-12T10:06:48.169226Z","iopub.status.idle":"2022-05-12T10:06:49.989077Z","shell.execute_reply.started":"2022-05-12T10:06:48.16917Z","shell.execute_reply":"2022-05-12T10:06:49.987977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kapsam grubu\ntarget_X = dt[dt['artis_durumu'].isnull() == False]['kapsam_grubu']\ntarget_y = dt[dt['artis_durumu'].isnull() == False]['artis_durumu']\nce_target = TargetEncoder(cols=['kapsam_grubu'], smoothing=8, min_samples_leaf=5).fit(target_X,target_y)\n\nce_output = ce_target.transform(dt['kapsam_grubu'].reset_index(drop=True))\ndt['kapsam_grubu_encoded'] = ce_output['kapsam_grubu']","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:07:32.853587Z","iopub.execute_input":"2022-05-12T10:07:32.854488Z","iopub.status.idle":"2022-05-12T10:07:35.450524Z","shell.execute_reply.started":"2022-05-12T10:07:32.85437Z","shell.execute_reply":"2022-05-12T10:07:35.449234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kapsam tipi\n# keep most frequents as they are and assign the others as others\nfocus_kapsam = dt['kapsam_tipi'].value_counts()[0:10].to_frame().reset_index()['index'].to_list()\ndt['kapsam_tipi_grouped'] = dt['kapsam_tipi'].copy()\ndt.loc[~dt['kapsam_tipi_grouped'].isin(focus_kapsam),'kapsam_tipi_grouped'] = \"other\"\n\nfrom sklearn.preprocessing import OneHotEncoder\nonehot_cols = ['kapsam_tipi_grouped']\n\nenc = OneHotEncoder()\n# transforming the column after fitting\nenc.fit(dt[onehot_cols])\nencoded_colm = enc.transform(dt[onehot_cols]).toarray()\n\ncol_ = [\"kapsam_tipi_grouped_\" + str(i) for i in range(11)]\nencoded_colm = pd.DataFrame(encoded_colm, columns = col_)\nencoded_colm\n# concatenating dataframes\ndt = pd.concat([dt, encoded_colm], axis = 1)\n\n# one-hot encoding\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keep most frequents as they are and assign the others as others\nfocus_kapsam = dt['office_id'].value_counts()[0:50].to_frame().reset_index()['index'].to_list()\ndt['office_id_grouped'] = dt['office_id'].copy()\ndt.loc[~dt['office_id_grouped'].isin(focus_kapsam),'office_id_grouped'] = 0\n\nfrom category_encoders import TargetEncoder\n#use target encoder to encode office_id but treat the most frequent 50 as original and the remaining as other\ntarget_X = dt[dt['artis_durumu'].isnull() == False]['office_id_grouped']\ntarget_y = dt[dt['artis_durumu'].isnull() == False]['artis_durumu']\nce_target = TargetEncoder(cols=['office_id_grouped'], smoothing=8, min_samples_leaf=5).fit(target_X,target_y)\n\nce_output = ce_target.transform(dt['office_id_grouped'].reset_index(drop=True))\ndt['office_id_grouped_encoded'] = ce_output['office_id_grouped']\n\n# target encoding","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Updating Numerical columns\n\n* sigorta_tip\n* dogum_tarihi\n* musteri_segmenti","metadata":{}},{"cell_type":"code","source":"dt['musteri_segmenti'] = dt['musteri_segmenti'].astype(int)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:31:49.967697Z","iopub.execute_input":"2022-05-12T10:31:49.968435Z","iopub.status.idle":"2022-05-12T10:31:50.167358Z","shell.execute_reply.started":"2022-05-12T10:31:49.968387Z","shell.execute_reply":"2022-05-12T10:31:50.166328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onehot_cols = ['sigorta_tip', 'musteri_segmenti']\n\nenc = OneHotEncoder()\n# transforming the column after fitting\nenc.fit(dt[onehot_cols])\n\nencoded_colm = enc.transform(dt[onehot_cols]).toarray()\nencoded_colm = pd.DataFrame(encoded_colm, columns = enc.get_feature_names_out(onehot_cols))\nencoded_colm\n\n# concatenating dataframes\ndt = pd.concat([dt, encoded_colm], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:32:00.970388Z","iopub.execute_input":"2022-05-12T10:32:00.97073Z","iopub.status.idle":"2022-05-12T10:32:02.703048Z","shell.execute_reply.started":"2022-05-12T10:32:00.970688Z","shell.execute_reply":"2022-05-12T10:32:02.701983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check Distributions**","metadata":{}},{"cell_type":"code","source":"print(plt.hist(dt[\"sigorta_tip\"]))\ndt.sigorta_tip.value_counts()\nprint(pd.crosstab(dt['sigorta_tip'],dt['artis_durumu']))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:12:14.180985Z","iopub.execute_input":"2022-05-12T10:12:14.181419Z","iopub.status.idle":"2022-05-12T10:12:14.436371Z","shell.execute_reply.started":"2022-05-12T10:12:14.18138Z","shell.execute_reply":"2022-05-12T10:12:14.435228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(plt.hist(dt[\"musteri_segmenti\"]))\ndt.musteri_segmenti.value_counts()\nprint(pd.crosstab(dt['musteri_segmenti'],dt['artis_durumu']))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:29:40.947379Z","iopub.execute_input":"2022-05-12T10:29:40.947791Z","iopub.status.idle":"2022-05-12T10:29:41.342307Z","shell.execute_reply.started":"2022-05-12T10:29:40.947754Z","shell.execute_reply":"2022-05-12T10:29:41.341238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(plt.hist(dt[\"dogum_tarihi\"]))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:18:44.094769Z","iopub.execute_input":"2022-05-12T10:18:44.095188Z","iopub.status.idle":"2022-05-12T10:18:44.330067Z","shell.execute_reply.started":"2022-05-12T10:18:44.095153Z","shell.execute_reply":"2022-05-12T10:18:44.328951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kwargs = dict(alpha=0.5, bins=100, density=True, stacked=True)\n\nx1 = dt.loc[dt.artis_durumu==1, 'dogum_tarihi']\nx2 = dt.loc[dt.artis_durumu==0, 'dogum_tarihi']\n\nplt.hist(x1, **kwargs, color='g', label='1')\nplt.hist(x2, **kwargs, color='b', label='0')\nplt.gca().set(title='Frequency Histogram ', ylabel='Frequency')\nplt.xlim(1919,2002)\nplt.legend();","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:27:18.253703Z","iopub.execute_input":"2022-05-12T10:27:18.254854Z","iopub.status.idle":"2022-05-12T10:27:19.027886Z","shell.execute_reply.started":"2022-05-12T10:27:18.254776Z","shell.execute_reply":"2022-05-12T10:27:19.026549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dt.to_csv(\"processed_data.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:34:08.922124Z","iopub.execute_input":"2022-05-12T10:34:08.922809Z","iopub.status.idle":"2022-05-12T10:35:27.808646Z","shell.execute_reply.started":"2022-05-12T10:34:08.922761Z","shell.execute_reply":"2022-05-12T10:35:27.807154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"#from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import early_stopping,log_evaluation\n#from imblearn.over_sampling import SMOTE,KMeansSMOTE\nfrom sklearn.preprocessing import StandardScaler\n\n\nSEED=22","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = pd.read_csv(\"../input/anadolu-hayat-emeklilik-datathon-coderspace/samplesubmission.csv\")\ndt = pd.read_csv(\"../input/processed-data2/processed_data2.csv\") # already prepared processed data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new features about artis_durumu\n\n# ilk uc ayda artmis mi artmamis mi\ndt['artis_durumu_2020'] = np.where((1.15*dt['ocak_vade_tutari'] < dt['subat_vade_tutari'])|(1.15*dt['ocak_vade_tutari'] < dt['mart_vade_tutari']) | (1.15*dt['subat_vade_tutari'] < dt['mart_vade_tutari']), 1,0) \n\n# number of artis\nmonths = [\"ocak\",\"subat\",\"mart\",\"nisan\",\"mayis\",\"haziran\",\"temmuz\",\"agustos\",\"eylul\",\"ekim\",\"kasim\",\"aralik\"]\nvade_cols = [str(i)+'_vade_tutari' for i in months]\n\nartis_number = [0] * dt.shape[0]\nfor j in range(12):\n    artis_number = artis_number + np.where(dt[vade_cols[j]]>(1.2*dt[vade_cols[j-1]]),1,0)\n    \ndt['num_of_artis'] = artis_number\n\n# correct values\ndt['sene_sonu_hesap_degeri']= np.where(dt['sene_sonu_hesap_degeri'] <1,0,dt['sene_sonu_hesap_degeri'])\ndt['sene_basi_hesap_degeri']= np.where(dt['sene_basi_hesap_degeri'] <1,0,dt['sene_basi_hesap_degeri'])\n\n#Q1 = dt['sene_basi_hesap_degeri'].quantile(0.25)#lower = (mn - 3*sd)\n#Q3 = dt['sene_basi_hesap_degeri'].quantile(0.75)#upper = (mn + 3*sd)\n#IQR = Q3-Q1\n#\n#lower = np.where((Q1 - 1.5*IQR)<0,0,Q1 - 1.5*IQR)\n#upper = Q3 + 1.5*IQR\n#\n#dt['sene_basi_hesap_degeri'] = np.where(dt['sene_basi_hesap_degeri']< lower, lower,dt['sene_basi_hesap_degeri'])\n#dt['sene_basi_hesap_degeri'] = np.where(dt['sene_basi_hesap_degeri']> upper, upper,dt['sene_basi_hesap_degeri'])\n\n#Q1 = dt['sene_sonu_hesap_degeri'].quantile(0.25)#lower = (mn - 3*sd)\n#Q3 = dt['sene_sonu_hesap_degeri'].quantile(0.75)#upper = (mn + 3*sd)\n#IQR = Q3-Q1\n#\n#lower = np.where((Q1 - 1.5*IQR)<0,0,Q1 - 1.5*IQR)\n#upper = Q3 + 1.5*IQR\n#\n#dt['sene_sonu_hesap_degeri'] = np.where(dt['sene_sonu_hesap_degeri']< lower, lower,dt['sene_sonu_hesap_degeri'])\n#dt['sene_sonu_hesap_degeri'] = np.where(dt['sene_sonu_hesap_degeri']> upper, upper,dt['sene_sonu_hesap_degeri'])\n\n# treat NAs and inf\ndt['hesap_degeri_degisimi_perc'] = (dt['sene_sonu_hesap_degeri'] - dt['sene_basi_hesap_degeri']) / dt['sene_basi_hesap_degeri']\ndt['hesap_degeri_degisimi_perc'].replace([np.inf, -np.inf], np.nan, inplace=True)\n#dt['hesap_degeri_degisimi_perc'].fillna(1, inplace =True)\ndt['hesap_degeri_degisimi_perc'].fillna(dt['hesap_degeri_degisimi_perc'].median(), inplace =True)\n\nQ1 = dt['hesap_degeri_degisimi_perc'].quantile(0.25)#lower = (mn - 3*sd)\nQ3 = dt['hesap_degeri_degisimi_perc'].quantile(0.75)#upper = (mn + 3*sd)\nIQR = Q3-Q1\n\nlower = np.where((Q1 - 1.5*IQR)<0,0,Q1 - 1.5*IQR)\nupper = Q3 + 1.5*IQR\n\ndt['hesap_degeri_degisimi_perc'] = np.where(dt['hesap_degeri_degisimi_perc']< lower, lower,dt['hesap_degeri_degisimi_perc'])\ndt['hesap_degeri_degisimi_perc'] = np.where(dt['hesap_degeri_degisimi_perc']> upper, upper,dt['hesap_degeri_degisimi_perc'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"code","source":"# takes model as an input\ndef get_optimal_cutoff(model,X_test,y_test):\n    optimal_cutoff = pd.DataFrame()\n    for i in range(20,80,5):\n        temp = np.where(pd.DataFrame(model.predict_proba(X_test))[1]<=i/100, 0, 1)\n        optimal_cutoff = pd.concat([optimal_cutoff,\n            pd.DataFrame(\n                [{\n                 'Cutoff': i/100,\n                 'f1': f1_score(y_test, temp)\n             }]\n            )]\n        )\n    del temp\n    optimal_cutoff.set_index('Cutoff', inplace = True)\n    return optimal_cutoff['f1'].idxmax()\n\n# takes probability as an input\ndef get_optimal_cutoff2(prob,X_test,y_test):\n    optimal_cutoff = pd.DataFrame()    \n    for i in range(10,91,5):\n        temp = np.where(pd.DataFrame(prob)[1]<=i/100, 0, 1)\n        optimal_cutoff = pd.concat([optimal_cutoff,\n            pd.DataFrame(\n                [{\n                 'Cutoff': i/100,\n                 'f1': f1_score(y_test, temp)\n             }]\n            )]\n        )\n    del temp\n    optimal_cutoff.set_index('Cutoff', inplace = True)\n    return optimal_cutoff['f1'].idxmax()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-23T20:32:06.997185Z","iopub.execute_input":"2022-05-23T20:32:06.997582Z","iopub.status.idle":"2022-05-23T20:32:07.007952Z","shell.execute_reply.started":"2022-05-23T20:32:06.997552Z","shell.execute_reply":"2022-05-23T20:32:07.007097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optimal ensemble for two models\ndef optimal_ensemble(prob1,prob2,y):\n    coef = []\n    for i in np.linspace(0, 1, 11):\n        if i<=1:\n            coef.append((i,1-i))\n            \n    optimal_coef = pd.DataFrame()\n    for i in coef:\n        c1, c2 = i\n        optimal_coef = optimal_coef.append(\n            pd.DataFrame(\n                {\n                    'c1': [c1],\n                    'c2': [c2],\n                    'roc': roc_auc_score(y,prob1*c1 + prob2*c2)\n                }\n            )\n        )\n        \n    c1,c2 = optimal_coef.iloc[optimal_coef['roc'].argmax()][['c1', 'c2']]\n    return c1,c2\n\n# optimal ensemble for three models\ndef optimal_ensemble3(prob1,prob2,prob3,y):\n    coef = []\n    for i in np.linspace(0, 1, 11):\n        for j in np.linspace(0, 1, 11):\n            if i+j<=1:\n                coef.append((i, j, 1-i-j))\n    \n    optimal_coef = pd.DataFrame()\n    for i in coef:\n        c1, c2, c3 = i\n        optimal_coef = optimal_coef.append(\n        pd.DataFrame(\n            {\n                'c1': [c1],\n                'c2': [c2],\n                'c3': [c3],\n                'roc': roc_auc_score(y,prob1*c1 + prob2*c2+ prob3*c3)\n            }\n        )\n    )\n        \n    c1, c2, c3 = optimal_coef.iloc[optimal_coef['roc'].argmax()][['c1', 'c2', 'c3']]\n    return c1,c2,c3","metadata":{"execution":{"iopub.status.busy":"2022-05-23T20:32:08.644914Z","iopub.execute_input":"2022-05-23T20:32:08.645504Z","iopub.status.idle":"2022-05-23T20:32:08.659009Z","shell.execute_reply.started":"2022-05-23T20:32:08.645473Z","shell.execute_reply":"2022-05-23T20:32:08.658082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def percentile(n):\n    def percentile_(x):\n        return np.percentile(x, n)\n    percentile_.__name__ = 'percentile_%s' % n\n    return percentile_","metadata":{"execution":{"iopub.status.busy":"2022-05-23T20:41:25.064985Z","iopub.execute_input":"2022-05-23T20:41:25.065390Z","iopub.status.idle":"2022-05-23T20:41:25.072209Z","shell.execute_reply.started":"2022-05-23T20:41:25.065355Z","shell.execute_reply":"2022-05-23T20:41:25.071386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling Approaches","metadata":{}},{"cell_type":"code","source":"from plotnine import *\nfrom mizani.formatters import percent_format\n\nfocus_kapsam = dt.kapsam_tipi.value_counts()[0:15].to_frame().reset_index()['index'].to_list()\n\nfocus_dt = dt[(dt['kapsam_tipi'].isin(focus_kapsam) == True) & (dt.artis_durumu.isnull() == False)]\n(ggplot(focus_dt)\n    + aes(x=focus_dt['kapsam_tipi'], fill = focus_dt['artis_durumu'].astype('category'))\n    + geom_bar(position = \"fill\") \n    + scale_y_continuous(labels=percent_format())#+ scale_y_continuous(labels=lambda l: [\"%d%%\" % (v * 100) for v in l])\n    + theme(figure_size=(20, 6))\n)\n\n#p = ggplot(mpg) + geom_bar(aes(x='manufacturer', fill='class'), position='fill')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-23T20:36:23.006935Z","iopub.execute_input":"2022-05-23T20:36:23.007463Z","iopub.status.idle":"2022-05-23T20:36:33.504700Z","shell.execute_reply.started":"2022-05-23T20:36:23.007429Z","shell.execute_reply":"2022-05-23T20:36:33.503117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"focus_kapsam = dt['office_id'].value_counts()[0:10].to_frame().reset_index()['index'].to_list()\n\nfocus_dt = dt[(dt['office_id'].isin(focus_kapsam) == True) & (dt.artis_durumu.isnull() == False)]\n(ggplot(focus_dt)\n    + aes(x=focus_dt['office_id'], fill = focus_dt['artis_durumu'].astype('category'))\n    + geom_bar(position = \"fill\") \n    + scale_y_continuous(labels=percent_format())#+ scale_y_continuous(labels=lambda l: [\"%d%%\" % (v * 100) for v in l])\n    + theme(figure_size=(20, 6))\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T20:36:53.176178Z","iopub.execute_input":"2022-05-23T20:36:53.177223Z","iopub.status.idle":"2022-05-23T20:36:54.670388Z","shell.execute_reply.started":"2022-05-23T20:36:53.177164Z","shell.execute_reply":"2022-05-23T20:36:54.669461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"focus_kapsam = dt['meslek'].value_counts()[0:15].to_frame().reset_index()['index'].to_list()\n\nfocus_dt = dt[(dt['meslek'].isin(focus_kapsam) == True) & (dt.artis_durumu.isnull() == False)]\n(ggplot(focus_dt)\n    + aes(x=focus_dt['meslek'], fill = focus_dt['artis_durumu'].astype('category'))\n    + geom_bar(position = \"fill\") \n    + scale_y_continuous(labels=percent_format())#+ scale_y_continuous(labels=lambda l: [\"%d%%\" % (v * 100) for v in l])\n    + theme(figure_size=(20, 6))\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T20:36:54.671826Z","iopub.execute_input":"2022-05-23T20:36:54.672810Z","iopub.status.idle":"2022-05-23T20:37:03.792232Z","shell.execute_reply.started":"2022-05-23T20:36:54.672775Z","shell.execute_reply":"2022-05-23T20:37:03.791059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Index _ Submodeling\n\nConstruct different models for sigorta_tip & musteri_segmenti pairs with sufficient number of observations","metadata":{}},{"cell_type":"code","source":"train = dt.loc[dt['artis_durumu'].isnull() == False]\n# drop na columns if there is any na values in it\ntrain = train.dropna(axis=1, how='any') # na cols \n\ntrain['artis_durumu'] = train['artis_durumu'].astype('category')\n\ntest = dt.loc[dt['artis_durumu'].isnull() == True]\ntest = test[train.columns].copy()\n\nmodeling_codes = pd.DataFrame(train.groupby(['musteri_segmenti','sigorta_tip'])['policy_id'].count()).reset_index().reset_index()\n#modeling_codes.sort_values('sigorta_tip')\n\n# sigorta_tip = 1 -- keep musteri_segmenti\n# sigorta_tip = 4 -- do not detail on musteri_segmenti / use musteri_segmenti= 102\n# sigorta_tip = 6 --  exclude 105 and 101\n# sigorta_tip = 7 -- ok\n# sigorta_tip = 8 -- do not detail on musteri_segmenti/ combine them\n\nmodeling_codes['index'] = np.NaN\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([1,7]), 'index']= range(1, 1+ len(modeling_codes[modeling_codes['sigorta_tip'].isin([1,7])]))\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([4]), 'index'] = 1 + modeling_codes['index'].max()\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([8]), 'index'] = 1 + modeling_codes['index'].max()\nmodeling_codes.loc[(modeling_codes['sigorta_tip'].isin([6])) & (modeling_codes['musteri_segmenti'].isin([101,105])), 'index'] = 1 + modeling_codes['index'].max()\nmodeling_codes.loc[modeling_codes['index'].isnull() == True, 'index'] = range(16,20)\nmodeling_codes.rename(columns = {'index':'model_index'}, inplace = True)\n\nmodeling_codes.loc[modeling_codes['model_index'] == 15,'model_index'] = 16\nmodeling_codes.loc[modeling_codes['model_index'] == 2,'model_index'] = 4\n\n\ntrain = train.merge(modeling_codes[['sigorta_tip','musteri_segmenti','model_index']], on = ['sigorta_tip','musteri_segmenti'], how = 'left')\ntest = test.merge(modeling_codes[['sigorta_tip','musteri_segmenti','model_index']], on = ['sigorta_tip','musteri_segmenti'], how = 'left')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Sets","metadata":{}},{"cell_type":"code","source":"#deneme1_new best\nfeatures = ['dogum_tarihi', 'cinsiyet', 'memleket', 'gelir', 'cocuk_sayisi','sene_basi_hesap_degeri', 'sene_sonu_hesap_degeri',\n            'year', 'month', \n            'ocak_odenen_tutar','subat_odenen_tutar','mart_odenen_tutar', 'nisan_odenen_tutar','mayis_odenen_tutar','haziran_odenen_tutar','temmuz_odenen_tutar','agustos_odenen_tutar','eylul_odenen_tutar','ekim_odenen_tutar','kasim_odenen_tutar','aralik_odenen_tutar',\n            'ocak_vade_tutari','subat_vade_tutari','mart_vade_tutari','nisan_vade_tutari','mayis_vade_tutari','haziran_vade_tutari','temmuz_vade_tutari','agustos_vade_tutari','eylul_vade_tutari','ekim_vade_tutari','kasim_vade_tutari','aralik_vade_tutari',\n            'ocak_odeme_orani', 'subat_odeme_orani', 'mart_odeme_orani', 'nisan_odeme_orani', 'mayis_odeme_orani', 'haziran_odeme_orani', 'temmuz_odeme_orani', 'agustos_odeme_orani', 'eylul_odeme_orani', 'ekim_odeme_orani', 'kasim_odeme_orani', 'aralik_odeme_orani', \n            'average_vade_tutari', 'min_vade_tutari', 'max_vade_tutari', 'change_in_vade_tutari', 'vade_vs_gelir', \n            'toplam_notpaid', 'hesap_degeri_degisimi','hesap_degeri_degisimi_perc', \n            'sozlesme_kokeni_NEW', 'sozlesme_kokeni_detay_NEW', #'uyruk_TR',\n            'medeni_hal_0', 'medeni_hal_1', \n            'yatirim_karakteri_Bilge', 'yatirim_karakteri_Cesur', 'yatirim_karakteri_Dengeli', 'yatirim_karakteri_Dikkatli', 'yatirim_karakteri_Temkinli', 'yatirim_karakteri_Yetkin',\n            'police_sehir_1', 'police_sehir_2', 'police_sehir_3', 'police_sehir_4', 'police_sehir_5', 'police_sehir_6', 'police_sehir_7', 'police_sehir_8', 'police_sehir_9', 'police_sehir_10', 'police_sehir_11', 'police_sehir_12', \n            'Kapsam_Tipi_1', 'Kapsam_Tipi_2', 'Kapsam_Tipi_3', 'Kapsam_Tipi_4', 'Kapsam_Tipi_5', 'Kapsam_Tipi_6', 'Kapsam_Tipi_7', 'Kapsam_Tipi_8', 'Kapsam_Tipi_9', 'Kapsam_Tipi_10', 'Kapsam_Tipi_11', 'Kapsam_Tipi_12', \n            'meslek_kirilim_encoded', 'dagitim_kanali_encoded', 'kapsam_grubu_encoded' ,\n            'odeme_ratio','trend_vade','trend_vade_odenen_fark','PD_OMEDI','PD_ODEDI','PY_OMEDI','PY_ODEDI','PA_OMEDI','PA_ODEDI',\n            'office_id_grouped_encoded',\n            'kapsam_tipi_grouped_0','kapsam_tipi_grouped_1','kapsam_tipi_grouped_2','kapsam_tipi_grouped_3','kapsam_tipi_grouped_4','kapsam_tipi_grouped_5','kapsam_tipi_grouped_6','kapsam_tipi_grouped_7','kapsam_tipi_grouped_8','kapsam_tipi_grouped_9','kapsam_tipi_grouped_10',\n            'office_mean_vade_tutari','vade_in_office',\n            'num_of_artis'\n           ]   \n\n## old best\n#features = ['dogum_tarihi', 'cinsiyet', 'memleket', 'gelir', 'cocuk_sayisi','sene_basi_hesap_degeri', 'sene_sonu_hesap_degeri',\n#            'year', 'month', \n#            'ocak_odeme_orani', 'subat_odeme_orani', 'mart_odeme_orani', 'nisan_odeme_orani', 'mayis_odeme_orani', 'haziran_odeme_orani', 'temmuz_odeme_orani', 'agustos_odeme_orani', 'eylul_odeme_orani', 'ekim_odeme_orani', 'kasim_odeme_orani', 'aralik_odeme_orani', \n#            'average_vade_tutari', 'min_vade_tutari', 'max_vade_tutari', 'change_in_vade_tutari', 'vade_vs_gelir', \n#            'toplam_notpaid', #'hesap_degeri_degisimi_perc', \n#            'sozlesme_kokeni_NEW', 'sozlesme_kokeni_detay_NEW', #'uyruk_TR',\n#            'medeni_hal_0', 'medeni_hal_1', \n#            'yatirim_karakteri_Bilge', 'yatirim_karakteri_Cesur', 'yatirim_karakteri_Dengeli', 'yatirim_karakteri_Dikkatli', 'yatirim_karakteri_Temkinli', 'yatirim_karakteri_Yetkin',\n#            'police_sehir_1', 'police_sehir_2', 'police_sehir_3', 'police_sehir_4', 'police_sehir_5', 'police_sehir_6', 'police_sehir_7', 'police_sehir_8', 'police_sehir_9', 'police_sehir_10', 'police_sehir_11', 'police_sehir_12', \n#            'Kapsam_Tipi_1', 'Kapsam_Tipi_2', 'Kapsam_Tipi_3', 'Kapsam_Tipi_4', 'Kapsam_Tipi_5', 'Kapsam_Tipi_6', 'Kapsam_Tipi_7', 'Kapsam_Tipi_8', 'Kapsam_Tipi_9', 'Kapsam_Tipi_10', 'Kapsam_Tipi_11', 'Kapsam_Tipi_12', \n#            'meslek_kirilim_encoded', 'dagitim_kanali_encoded', 'kapsam_grubu_encoded' \n#            #'sigorta_tip_1', 'sigorta_tip_4', 'sigorta_tip_6', 'sigorta_tip_7', 'sigorta_tip_8', \n#            #'musteri_segmenti_101', 'musteri_segmenti_102', 'musteri_segmenti_103', 'musteri_segmenti_104', 'musteri_segmenti_105', 'musteri_segmenti_106'\n#           ]\n\n# worse\n#features = [#'dogum_tarihi', 'cinsiyet', 'memleket', \n#            'gelir', 'cocuk_sayisi','sene_basi_hesap_degeri', 'sene_sonu_hesap_degeri',\n#            'year', 'month', \n#            #'ocak_odeme_orani', 'subat_odeme_orani', 'mart_odeme_orani', 'nisan_odeme_orani', 'mayis_odeme_orani', 'haziran_odeme_orani', 'temmuz_odeme_orani', 'agustos_odeme_orani', 'eylul_odeme_orani', 'ekim_odeme_orani', 'kasim_odeme_orani', 'aralik_odeme_orani', \n#            'aralik_odeme_orani',\n#            'average_vade_tutari', 'min_vade_tutari', 'max_vade_tutari', 'change_in_vade_tutari', 'vade_vs_gelir', \n#            'toplam_notpaid', 'hesap_degeri_degisimi',#'hesap_degeri_degisimi_perc', \n#            'sozlesme_kokeni_NEW', 'sozlesme_kokeni_detay_NEW', #'uyruk_TR',\n#            'medeni_hal_0', 'medeni_hal_1', \n#            #'yatirim_karakteri_Bilge', 'yatirim_karakteri_Cesur', 'yatirim_karakteri_Dengeli', 'yatirim_karakteri_Dikkatli', 'yatirim_karakteri_Temkinli', 'yatirim_karakteri_Yetkin',\n#            #'police_sehir_1', 'police_sehir_2', 'police_sehir_3', 'police_sehir_4', 'police_sehir_5', 'police_sehir_6', 'police_sehir_7', 'police_sehir_8', 'police_sehir_9', 'police_sehir_10', 'police_sehir_11', 'police_sehir_12', \n#            'Kapsam_Tipi_1', 'Kapsam_Tipi_2', 'Kapsam_Tipi_3', 'Kapsam_Tipi_4', 'Kapsam_Tipi_5', 'Kapsam_Tipi_6', 'Kapsam_Tipi_7', 'Kapsam_Tipi_8', 'Kapsam_Tipi_9', 'Kapsam_Tipi_10', 'Kapsam_Tipi_11', 'Kapsam_Tipi_12', \n#            #'meslek_kirilim_encoded', 'dagitim_kanali_encoded', 'kapsam_grubu_encoded' ,\n#            'odeme_ratio','trend_vade','trend_vade_odenen_fark','PD_OMEDI','PD_ODEDI','PY_OMEDI','PY_ODEDI','PA_OMEDI','PA_ODEDI',\n#            #'office_id_grouped_encoded',\n#            #'kapsam_tipi_grouped_0','kapsam_tipi_grouped_1','kapsam_tipi_grouped_2','kapsam_tipi_grouped_3','kapsam_tipi_grouped_4','kapsam_tipi_grouped_5','kapsam_tipi_grouped_6','kapsam_tipi_grouped_7','kapsam_tipi_grouped_8','kapsam_tipi_grouped_9','kapsam_tipi_grouped_10',\n#            'office_mean_vade_tutari','vade_in_office',\n#            'num_of_artis', 'artis_durumu_2020'\n#            # the last two made the performance worse\n#            #'sigorta_tip_1', 'sigorta_tip_4', 'sigorta_tip_6', 'sigorta_tip_7', 'sigorta_tip_8', \n#            #'musteri_segmenti_101', 'musteri_segmenti_102', 'musteri_segmenti_103', 'musteri_segmenti_104', 'musteri_segmenti_105', 'musteri_segmenti_106'\n#           ]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(train.loc[:, (train.dtypes == int) | (train.dtypes == float)].columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2022-05-12T10:40:44.301619Z","iopub.execute_input":"2022-05-12T10:40:44.301881Z","iopub.status.idle":"2022-05-12T10:40:44.636436Z","shell.execute_reply.started":"2022-05-12T10:40:44.301826Z","shell.execute_reply":"2022-05-12T10:40:44.635303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best Model Config","metadata":{}},{"cell_type":"code","source":"## best config\n# lgbm & rf & opt ensemble.. new features .. \ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 200,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train,eval_metric = \"auc\",\n                 eval_set=[(X_train,y_train),(X_test,y_test)],\n                 callbacks=[\n                            log_evaluation(period=10)]) #early_stopping(stopping_rounds=50, first_metric_only=True),\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train,y_train)\n    \n    prob1 = lgbm_fit.predict_proba(X_test)\n    prob2 = lr_fit.predict_proba(X_test)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    \n    c1,c2 = optimal_ensemble(prob1,prob2,y_test)\n    prob = c1*prob1+c2*prob2\n    \n    cutoff = get_optimal_cutoff2(pd.DataFrame(prob)[1], X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n    \n    test_preds = pd.concat([test_preds,\n                       pd.DataFrame({ \n                           'model_index': i,\n                           'actual':y_test,\n                           'preds' :y_test_pred})]\n                      , ignore_index=True)\n    \n    #train performance\n    prob1 = lgbm_fit.predict_proba(X_train)\n    prob2 = lr_fit.predict_proba(X_train)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    prob = c1*prob1+c2*prob2\n    \n    y_train_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    train_f1_cutoff = f1_score(y_train,y_train_pred)\n    print(\"model_index \" + str(i) + \" : train_f1_score= \" + str(train_f1_cutoff))\n\n\n    ## submission preds\n    prob1 = lgbm_fit.predict_proba(sub_test[features])\n    prob2 = lr_fit.predict_proba(sub_test[features]) \n    prob = c1*prob1+c2*prob2\n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-22T10:32:45.053178Z","iopub.execute_input":"2022-05-22T10:32:45.053505Z","iopub.status.idle":"2022-05-22T10:43:25.090279Z","shell.execute_reply.started":"2022-05-22T10:32:45.053463Z","shell.execute_reply":"2022-05-22T10:43:25.089399Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(test_preds.actual,test_preds.preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T10:50:51.033374Z","iopub.execute_input":"2022-05-22T10:50:51.034104Z","iopub.status.idle":"2022-05-22T10:50:51.10814Z","shell.execute_reply.started":"2022-05-22T10:50:51.03406Z","shell.execute_reply":"2022-05-22T10:50:51.107242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = lr_fit.feature_importances_\n#\n# Sort the feature importance in descending order\n#\nsorted_indices = np.argsort(importances)[::-1]\n\nfig = plt.gcf()\nfig.set_size_inches(18, 8)\n\nplt.title('Feature Importance')\nplt.bar(range(X_train.shape[1]), importances[sorted_indices], align='center')\nplt.xticks(range(X_train.shape[1]), X_train.columns[sorted_indices], rotation=90)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T10:50:48.406525Z","iopub.execute_input":"2022-05-22T10:50:48.406811Z","iopub.status.idle":"2022-05-22T10:50:48.883604Z","shell.execute_reply.started":"2022-05-22T10:50:48.406783Z","shell.execute_reply":"2022-05-22T10:50:48.882949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best model config but with third model","metadata":{}},{"cell_type":"code","source":"dt.drop(columns = 'model_index', inplace = True)\n\nmodeling_codes = pd.DataFrame(dt.groupby(['musteri_segmenti','sigorta_tip'])['policy_id'].count()).reset_index().reset_index()\n#modeling_codes.sort_values('sigorta_tip')\n\n# sigorta_tip = 1 -- keep musteri_segmenti\n# sigorta_tip = 4 -- do not detail on musteri_segmenti / use musteri_segmenti= 102\n# sigorta_tip = 6 --  exclude 105 and 101\n# sigorta_tip = 7 -- ok\n# sigorta_tip = 8 -- do not detail on musteri_segmenti/ combine them\nmodeling_codes.rename(columns = {'index':'model_index'}, inplace = True)\n\nmodeling_codes['model_index'] = np.NaN\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([1,7]), 'model_index']= range(1, 1+ len(modeling_codes[modeling_codes['sigorta_tip'].isin([1,7])]))\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([4]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([8]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[(modeling_codes['sigorta_tip'].isin([6])) & (modeling_codes['musteri_segmenti'].isin([101,105])), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['model_index'].isnull() == True, 'model_index'] = range(16,20)\nmodeling_codes.loc[modeling_codes['model_index'] == 15,'model_index'] = 16\nmodeling_codes.loc[modeling_codes['model_index'] == 2,'model_index'] = 4\n\n##\ndt = dt.merge(modeling_codes[['sigorta_tip','musteri_segmenti','model_index']], on = ['sigorta_tip','musteri_segmenti'])","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:12:53.163175Z","iopub.execute_input":"2022-05-22T14:12:53.163463Z","iopub.status.idle":"2022-05-22T14:13:01.850182Z","shell.execute_reply.started":"2022-05-22T14:12:53.163434Z","shell.execute_reply":"2022-05-22T14:13:01.8491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = dt.loc[dt['artis_durumu'].isnull() == False]\n# drop na columns if there is any na values in it\ntrain = train.dropna(axis=1, how='any') # na cols \n\ntrain['artis_durumu'] = train['artis_durumu'].astype('category')\n\ntest = dt.loc[dt['artis_durumu'].isnull() == True]\ntest = test[train.columns].copy()\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:13:01.851903Z","iopub.execute_input":"2022-05-22T14:13:01.852146Z","iopub.status.idle":"2022-05-22T14:13:05.437509Z","shell.execute_reply.started":"2022-05-22T14:13:01.852115Z","shell.execute_reply":"2022-05-22T14:13:05.436489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## best config\n# lgbm & rf & xgboost with weights ~ opt ensemble.. reduced features .. \ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    weights_train = [0]* len(y_train)\n    percc = pd.DataFrame(y_train.value_counts(normalize = True)).reset_index()\n    w = round(percc.iloc[0,1]/percc.iloc[1,1],0)\n    w = max(0, w-1)\n    weights_train = np.where(y_train == 1,w,1)\n    \n    xgb_fit = XGBClassifier(objective='binary:logistic',\n                            max_depth= 3,\n                            learning_rate= 0.01,\n                            n_estimators=1000,\n                            eval_metric='auc',random_state=0)\n    \n    xgb_fit.fit(X_train, y_train,sample_weight=weights_train)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 200,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train) #early_stopping(stopping_rounds=50, first_metric_only=True),\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train,y_train)\n    \n    \n    \n    prob1 = lgbm_fit.predict_proba(X_test)\n    prob2 = lr_fit.predict_proba(X_test)\n    prob3 = xgb_fit.predict_proba(X_test)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    prob3 = pd.DataFrame(prob3)[1]\n    \n    c1,c2,c3 = optimal_ensemble3(prob1,prob2,prob3,y_test)\n    prob = c1*prob1+c2*prob2+c3*prob3\n    \n    cutoff = get_optimal_cutoff2(pd.DataFrame(prob)[1], X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n    \n    test_preds = pd.concat([test_preds,\n                       pd.DataFrame({ \n                           'model_index': i,\n                           'actual':y_test,\n                           'preds' :y_test_pred})]\n                      , ignore_index=True)\n    \n    #train performance\n    prob1 = lgbm_fit.predict_proba(X_train)\n    prob2 = lr_fit.predict_proba(X_train)\n    prob3 = lr_fit.predict_proba(X_train)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    prob3 = pd.DataFrame(prob3)[1]\n    prob = c1*prob1+c2*prob2+c3*prob3\n    \n    y_train_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    train_f1_cutoff = f1_score(y_train,y_train_pred)\n    print(\"model_index \" + str(i) + \" : train_f1_score= \" + str(train_f1_cutoff))\n\n\n    ## submission preds\n    prob1 = lgbm_fit.predict_proba(sub_test[features])\n    prob2 = lr_fit.predict_proba(sub_test[features]) \n    prob3 = xgb_fit.predict_proba(sub_test[features]) \n    prob = c1*prob1+c2*prob2+c3*prob3\n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:13:20.463353Z","iopub.execute_input":"2022-05-22T14:13:20.46363Z","iopub.status.idle":"2022-05-22T14:38:49.141073Z","shell.execute_reply.started":"2022-05-22T14:13:20.463602Z","shell.execute_reply":"2022-05-22T14:38:49.140452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(test_preds.actual,test_preds.preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:41:38.677766Z","iopub.execute_input":"2022-05-22T14:41:38.678075Z","iopub.status.idle":"2022-05-22T14:41:38.752042Z","shell.execute_reply.started":"2022-05-22T14:41:38.678046Z","shell.execute_reply":"2022-05-22T14:41:38.751101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best model config with a special treatment for index 9 and 11 \n\nwith a deeper level: max_vade_tutarı groups","metadata":{}},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.drop(columns = 'model_index', inplace = True)\n\nmodeling_codes = pd.DataFrame(dt.groupby(['musteri_segmenti','sigorta_tip'])['policy_id'].count()).reset_index().reset_index()\n#modeling_codes.sort_values('sigorta_tip')\n\n# sigorta_tip = 1 -- keep musteri_segmenti\n# sigorta_tip = 4 -- do not detail on musteri_segmenti / use musteri_segmenti= 102\n# sigorta_tip = 6 --  exclude 105 and 101\n# sigorta_tip = 7 -- ok\n# sigorta_tip = 8 -- do not detail on musteri_segmenti/ combine them\nmodeling_codes.rename(columns = {'index':'model_index'}, inplace = True)\n\nmodeling_codes['model_index'] = np.NaN\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([1,7]), 'model_index']= range(1, 1+ len(modeling_codes[modeling_codes['sigorta_tip'].isin([1,7])]))\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([4]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([8]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[(modeling_codes['sigorta_tip'].isin([6])) & (modeling_codes['musteri_segmenti'].isin([101,105])), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['model_index'].isnull() == True, 'model_index'] = range(16,20)\nmodeling_codes.loc[modeling_codes['model_index'] == 15,'model_index'] = 16\nmodeling_codes.loc[modeling_codes['model_index'] == 2,'model_index'] = 4\n\n##\ndt = dt.merge(modeling_codes[['sigorta_tip','musteri_segmenti','model_index']], on = ['sigorta_tip','musteri_segmenti'])","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:15:50.873461Z","iopub.execute_input":"2022-05-22T13:15:50.874243Z","iopub.status.idle":"2022-05-22T13:15:58.529023Z","shell.execute_reply.started":"2022-05-22T13:15:50.874207Z","shell.execute_reply":"2022-05-22T13:15:58.527967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt['max_vade_group']= dt.groupby(['sigorta_tip','musteri_segmenti'])['hesap_degeri_degisimi'].transform(\n                     lambda x: pd.qcut(x, 3, duplicates = 'drop', labels=False))","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:15:58.530582Z","iopub.execute_input":"2022-05-22T13:15:58.530836Z","iopub.status.idle":"2022-05-22T13:15:58.808394Z","shell.execute_reply.started":"2022-05-22T13:15:58.530806Z","shell.execute_reply":"2022-05-22T13:15:58.807582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt[dt.model_index == 9]['max_vade_group'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:15:58.809649Z","iopub.execute_input":"2022-05-22T13:15:58.809865Z","iopub.status.idle":"2022-05-22T13:16:00.269589Z","shell.execute_reply.started":"2022-05-22T13:15:58.80984Z","shell.execute_reply":"2022-05-22T13:16:00.26873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 106) & (dt.max_vade_group == 0.0),'model_index'] = 20\ndt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 106) & (dt.max_vade_group == 1.0),'model_index'] = 21\ndt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 106) & (dt.max_vade_group == 2.0),'model_index'] = 22\n#dt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 106) & (dt.max_vade_group == 3.0),'model_index'] = 23\n#dt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 106) & (dt.max_vade_group == 4.0),'model_index'] = 24\n\ndt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 105) & (dt.max_vade_group == 0.0),'model_index'] = 30\ndt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 105) & (dt.max_vade_group == 1.0),'model_index'] = 31\ndt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 105) & (dt.max_vade_group == 2.0),'model_index'] = 32\n#dt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 105) & (dt.max_vade_group == 3.0),'model_index'] = 33\n#dt.loc[(dt.sigorta_tip == 1) & (dt.musteri_segmenti == 105) & (dt.max_vade_group == 4.0),'model_index'] = 34","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:16:09.916781Z","iopub.execute_input":"2022-05-22T13:16:09.917574Z","iopub.status.idle":"2022-05-22T13:16:09.989332Z","shell.execute_reply.started":"2022-05-22T13:16:09.917516Z","shell.execute_reply":"2022-05-22T13:16:09.988329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt['model_index'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:16:13.483607Z","iopub.execute_input":"2022-05-22T13:16:13.483878Z","iopub.status.idle":"2022-05-22T13:16:13.501782Z","shell.execute_reply.started":"2022-05-22T13:16:13.483849Z","shell.execute_reply":"2022-05-22T13:16:13.501139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = dt.loc[dt['artis_durumu'].isnull() == False]\n# drop na columns if there is any na values in it\ntrain = train.dropna(axis=1, how='any') # na cols \n\ntrain['artis_durumu'] = train['artis_durumu'].astype('category')\n\ntest = dt.loc[dt['artis_durumu'].isnull() == True]\ntest = test[train.columns].copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:16:24.347863Z","iopub.execute_input":"2022-05-22T13:16:24.348309Z","iopub.status.idle":"2022-05-22T13:16:26.43156Z","shell.execute_reply.started":"2022-05-22T13:16:24.348265Z","shell.execute_reply":"2022-05-22T13:16:26.430774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## best config\n# lgbm & rf & opt ensemble.. new features .. \ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 200,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train) #early_stopping(stopping_rounds=50, first_metric_only=True),\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train,y_train)\n    \n    prob1 = lgbm_fit.predict_proba(X_test)\n    prob2 = lr_fit.predict_proba(X_test)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    \n    c1,c2 = optimal_ensemble(prob1,prob2,y_test)\n    prob = c1*prob1+c2*prob2\n    \n    cutoff = get_optimal_cutoff2(pd.DataFrame(prob)[1], X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n    \n    test_preds = pd.concat([test_preds,\n                       pd.DataFrame({ \n                           'model_index': i,\n                           'actual':y_test,\n                           'preds' :y_test_pred})]\n                      , ignore_index=True)\n    \n    #train performance\n    prob1 = lgbm_fit.predict_proba(X_train)\n    prob2 = lr_fit.predict_proba(X_train)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    prob = c1*prob1+c2*prob2\n    \n    y_train_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    train_f1_cutoff = f1_score(y_train,y_train_pred)\n    print(\"model_index \" + str(i) + \" : train_f1_score= \" + str(train_f1_cutoff))\n\n\n    ## submission preds\n    prob1 = lgbm_fit.predict_proba(sub_test[features])\n    prob2 = lr_fit.predict_proba(sub_test[features]) \n    prob = c1*prob1+c2*prob2\n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:24:57.478172Z","iopub.execute_input":"2022-05-22T13:24:57.478783Z","iopub.status.idle":"2022-05-22T13:35:27.478697Z","shell.execute_reply.started":"2022-05-22T13:24:57.478733Z","shell.execute_reply":"2022-05-22T13:35:27.477527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(test_preds.actual,test_preds.preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T13:35:27.480627Z","iopub.execute_input":"2022-05-22T13:35:27.480942Z","iopub.status.idle":"2022-05-22T13:35:27.552534Z","shell.execute_reply.started":"2022-05-22T13:35:27.4809Z","shell.execute_reply":"2022-05-22T13:35:27.551576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Focus on another column for submodels ","metadata":{}},{"cell_type":"markdown","source":"### Based on Hesap degisim tutarı","metadata":{}},{"cell_type":"code","source":"pd.qcut(x=dt['hesap_degeri_degisimi'], q = 20, labels=range(20)).value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:39:09.188584Z","iopub.execute_input":"2022-05-22T11:39:09.188908Z","iopub.status.idle":"2022-05-22T11:39:09.333894Z","shell.execute_reply.started":"2022-05-22T11:39:09.188875Z","shell.execute_reply":"2022-05-22T11:39:09.332915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hesap degisim degeri yaramadi\ndt['model_index'] = pd.qcut(x=dt['max_vade_tutari'], q = 10, labels=range(10))\n\ntrain = dt.loc[dt['artis_durumu'].isnull() == False]\n# drop na columns if there is any na values in it\ntrain = train.dropna(axis=1, how='any') # na cols \n\ntrain['artis_durumu'] = train['artis_durumu'].astype('category')\n\ntest = dt.loc[dt['artis_durumu'].isnull() == True]\ntest = test[train.columns].copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:00:09.583977Z","iopub.execute_input":"2022-05-22T12:00:09.584315Z","iopub.status.idle":"2022-05-22T12:00:11.921475Z","shell.execute_reply.started":"2022-05-22T12:00:09.584282Z","shell.execute_reply":"2022-05-22T12:00:11.920562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## best config\n# lgbm & rf & opt ensemble.. new features .. \ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 200,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train) #early_stopping(stopping_rounds=50, first_metric_only=True),\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train,y_train)\n    \n    prob1 = lgbm_fit.predict_proba(X_test)\n    prob2 = lr_fit.predict_proba(X_test)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    \n    c1,c2 = optimal_ensemble(prob1,prob2,y_test)\n    prob = c1*prob1+c2*prob2\n    \n    cutoff = get_optimal_cutoff2(pd.DataFrame(prob)[1], X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n    \n    test_preds = pd.concat([test_preds,\n                       pd.DataFrame({ \n                           'model_index': i,\n                           'actual':y_test,\n                           'preds' :y_test_pred})]\n                      , ignore_index=True)\n    \n    #train performance\n    prob1 = lgbm_fit.predict_proba(X_train)\n    prob2 = lr_fit.predict_proba(X_train)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    prob = c1*prob1+c2*prob2\n    \n    y_train_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    train_f1_cutoff = f1_score(y_train,y_train_pred)\n    print(\"model_index \" + str(i) + \" : train_f1_score= \" + str(train_f1_cutoff))\n\n\n    ## submission preds\n    prob1 = lgbm_fit.predict_proba(sub_test[features])\n    prob2 = lr_fit.predict_proba(sub_test[features]) \n    prob = c1*prob1+c2*prob2\n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:00:13.705491Z","iopub.execute_input":"2022-05-22T12:00:13.70614Z","iopub.status.idle":"2022-05-22T12:11:00.842737Z","shell.execute_reply.started":"2022-05-22T12:00:13.706089Z","shell.execute_reply":"2022-05-22T12:11:00.841809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(test_preds.actual,test_preds.preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T12:11:00.844363Z","iopub.execute_input":"2022-05-22T12:11:00.844606Z","iopub.status.idle":"2022-05-22T12:11:00.91257Z","shell.execute_reply.started":"2022-05-22T12:11:00.84458Z","shell.execute_reply":"2022-05-22T12:11:00.911555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Based on Hesap degisim perc group\n\nreduced feature set ile full feature set arasinda net bir fark yok","metadata":{}},{"cell_type":"code","source":"dt['median_perc'] = dt.groupby(['sigorta_tip','musteri_segmenti'])['hesap_degeri_degisimi'].transform('median')\ndt['hesap_degisim_group'] = np.where(dt['hesap_degeri_degisimi'] < dt['median_perc'], 0,1)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:22:49.005982Z","iopub.execute_input":"2022-05-22T11:22:49.00628Z","iopub.status.idle":"2022-05-22T11:22:49.085396Z","shell.execute_reply.started":"2022-05-22T11:22:49.006248Z","shell.execute_reply":"2022-05-22T11:22:49.084441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = dt.loc[dt['artis_durumu'].isnull() == False]\n# drop na columns if there is any na values in it\ntrain = train.dropna(axis=1, how='any') # na cols \n\ntrain['artis_durumu'] = train['artis_durumu'].astype('category')\n\ntest = dt.loc[dt['artis_durumu'].isnull() == True]\ntest = test[train.columns].copy()\n\nmodeling_codes = pd.DataFrame(train.groupby(['musteri_segmenti','sigorta_tip'])['policy_id'].count()).reset_index().reset_index()\n#modeling_codes.sort_values('sigorta_tip')\n\n# sigorta_tip = 1 -- keep musteri_segmenti\n# sigorta_tip = 4 -- do not detail on musteri_segmenti / use musteri_segmenti= 102\n# sigorta_tip = 6 --  exclude 105 and 101\n# sigorta_tip = 7 -- ok\n# sigorta_tip = 8 -- do not detail on musteri_segmenti/ combine them\nmodeling_codes.rename(columns = {'index':'model_index'}, inplace = True)\n\nmodeling_codes['model_index'] = np.NaN\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([1,7]), 'model_index']= range(1, 1+ len(modeling_codes[modeling_codes['sigorta_tip'].isin([1,7])]))\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([4]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([8]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[(modeling_codes['sigorta_tip'].isin([6])) & (modeling_codes['musteri_segmenti'].isin([101,105])), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['model_index'].isnull() == True, 'model_index'] = range(16,20)\nmodeling_codes.loc[modeling_codes['model_index'] == 15,'model_index'] = 16\nmodeling_codes.loc[modeling_codes['model_index'] == 2,'model_index'] = 4\n##\nmodeling_codes2 = pd.DataFrame(train.groupby(['sigorta_tip','musteri_segmenti','hesap_degisim_group'])['policy_id'].count()).reset_index().reset_index()\nmodeling_codes2 = modeling_codes2.sort_values('policy_id', ascending = False)\nmodeling_codes2 = modeling_codes2.merge(modeling_codes[['sigorta_tip','musteri_segmenti','model_index']], on = ['sigorta_tip','musteri_segmenti'])\n\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11)& (modeling_codes2['hesap_degisim_group'] == 1), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 7) & (modeling_codes2['hesap_degisim_group'] == 1), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 9) & (modeling_codes2['hesap_degisim_group'] == 1), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 5) & (modeling_codes2['hesap_degisim_group'] == 1), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 3) & (modeling_codes2['hesap_degisim_group'] == 1), 'model_index'] = 1 + max(modeling_codes2['model_index'])\n\ntrain = train.merge(modeling_codes2[['sigorta_tip','musteri_segmenti','hesap_degisim_group','model_index']], on = ['sigorta_tip','musteri_segmenti','hesap_degisim_group'], how = 'left')\ntest = test.merge(modeling_codes2[['sigorta_tip','musteri_segmenti','hesap_degisim_group','model_index']], on = ['sigorta_tip','musteri_segmenti','hesap_degisim_group'], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:22:49.621163Z","iopub.execute_input":"2022-05-22T11:22:49.62164Z","iopub.status.idle":"2022-05-22T11:22:58.147576Z","shell.execute_reply.started":"2022-05-22T11:22:49.621601Z","shell.execute_reply":"2022-05-22T11:22:58.146683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features = ['dogum_tarihi', 'cinsiyet', 'memleket', 'gelir', 'cocuk_sayisi','sene_basi_hesap_degeri', 'sene_sonu_hesap_degeri',\n            'year', 'month', \n            'ocak_odenen_tutar','subat_odenen_tutar','mart_odenen_tutar', 'nisan_odenen_tutar','mayis_odenen_tutar','haziran_odenen_tutar','temmuz_odenen_tutar','agustos_odenen_tutar','eylul_odenen_tutar','ekim_odenen_tutar','kasim_odenen_tutar','aralik_odenen_tutar',\n            'ocak_vade_tutari','subat_vade_tutari','mart_vade_tutari','nisan_vade_tutari','mayis_vade_tutari','haziran_vade_tutari','temmuz_vade_tutari','agustos_vade_tutari','eylul_vade_tutari','ekim_vade_tutari','kasim_vade_tutari','aralik_vade_tutari',\n            'ocak_odeme_orani', 'subat_odeme_orani', 'mart_odeme_orani', 'nisan_odeme_orani', 'mayis_odeme_orani', 'haziran_odeme_orani', 'temmuz_odeme_orani', 'agustos_odeme_orani', 'eylul_odeme_orani', 'ekim_odeme_orani', 'kasim_odeme_orani', 'aralik_odeme_orani', \n            'average_vade_tutari', 'min_vade_tutari', 'max_vade_tutari', 'change_in_vade_tutari', 'vade_vs_gelir', \n            'toplam_notpaid', 'hesap_degeri_degisimi','hesap_degeri_degisimi_perc', \n            'sozlesme_kokeni_NEW', 'sozlesme_kokeni_detay_NEW', #'uyruk_TR',\n            'medeni_hal_0', 'medeni_hal_1', \n            'yatirim_karakteri_Bilge', 'yatirim_karakteri_Cesur', 'yatirim_karakteri_Dengeli', 'yatirim_karakteri_Dikkatli', 'yatirim_karakteri_Temkinli', 'yatirim_karakteri_Yetkin',\n            'police_sehir_1', 'police_sehir_2', 'police_sehir_3', 'police_sehir_4', 'police_sehir_5', 'police_sehir_6', 'police_sehir_7', 'police_sehir_8', 'police_sehir_9', 'police_sehir_10', 'police_sehir_11', 'police_sehir_12', \n            'Kapsam_Tipi_1', 'Kapsam_Tipi_2', 'Kapsam_Tipi_3', 'Kapsam_Tipi_4', 'Kapsam_Tipi_5', 'Kapsam_Tipi_6', 'Kapsam_Tipi_7', 'Kapsam_Tipi_8', 'Kapsam_Tipi_9', 'Kapsam_Tipi_10', 'Kapsam_Tipi_11', 'Kapsam_Tipi_12', \n            'meslek_kirilim_encoded', 'dagitim_kanali_encoded', 'kapsam_grubu_encoded' ,\n            'odeme_ratio','trend_vade','trend_vade_odenen_fark','PD_OMEDI','PD_ODEDI','PY_OMEDI','PY_ODEDI','PA_OMEDI','PA_ODEDI',\n            'office_id_grouped_encoded',\n            'kapsam_tipi_grouped_0','kapsam_tipi_grouped_1','kapsam_tipi_grouped_2','kapsam_tipi_grouped_3','kapsam_tipi_grouped_4','kapsam_tipi_grouped_5','kapsam_tipi_grouped_6','kapsam_tipi_grouped_7','kapsam_tipi_grouped_8','kapsam_tipi_grouped_9','kapsam_tipi_grouped_10',\n            'office_mean_vade_tutari','vade_in_office',\n            'num_of_artis'\n           ]  ","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:22:58.149654Z","iopub.execute_input":"2022-05-22T11:22:58.14997Z","iopub.status.idle":"2022-05-22T11:22:58.158645Z","shell.execute_reply.started":"2022-05-22T11:22:58.149929Z","shell.execute_reply":"2022-05-22T11:22:58.1577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## best config\n# lgbm & rf & opt ensemble.. new features .. \ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 200,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train)\n                 #,eval_metric = \"auc\",\n                 #eval_set=[(X_train,y_train),(X_test,y_test)],\n                 #callbacks=[log_evaluation(period=10)]) #early_stopping(stopping_rounds=50, first_metric_only=True),\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train,y_train)\n    \n    prob1 = lgbm_fit.predict_proba(X_test)\n    prob2 = lr_fit.predict_proba(X_test)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    \n    c1,c2 = optimal_ensemble(prob1,prob2,y_test)\n    prob = c1*prob1+c2*prob2\n    \n    cutoff = get_optimal_cutoff2(pd.DataFrame(prob)[1], X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n    \n    test_preds = pd.concat([test_preds,\n                       pd.DataFrame({ \n                           'model_index': i,\n                           'actual':y_test,\n                           'preds' :y_test_pred})]\n                      , ignore_index=True)\n    \n    #train performance\n    prob1 = lgbm_fit.predict_proba(X_train)\n    prob2 = lr_fit.predict_proba(X_train)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    prob = c1*prob1+c2*prob2\n    \n    y_train_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    train_f1_cutoff = f1_score(y_train,y_train_pred)\n    print(\"model_index \" + str(i) + \" : train_f1_score= \" + str(train_f1_cutoff))\n\n\n    ## submission preds\n    prob1 = lgbm_fit.predict_proba(sub_test[features])\n    prob2 = lr_fit.predict_proba(sub_test[features]) \n    prob = c1*prob1+c2*prob2\n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:23:15.241479Z","iopub.execute_input":"2022-05-22T11:23:15.241746Z","iopub.status.idle":"2022-05-22T11:30:36.529352Z","shell.execute_reply.started":"2022-05-22T11:23:15.24172Z","shell.execute_reply":"2022-05-22T11:30:36.52855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(test_preds.actual,test_preds.preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T11:08:20.622254Z","iopub.execute_input":"2022-05-22T11:08:20.622702Z","iopub.status.idle":"2022-05-22T11:08:20.696061Z","shell.execute_reply.started":"2022-05-22T11:08:20.622667Z","shell.execute_reply":"2022-05-22T11:08:20.695193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Elim\n\nNOT APPLIED DUE TO VEERY LONG RUN DURATIONS","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n## rfe added - but takes too much time\n# lgbm & rf & opt ensemble.. new features .. \ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in [4]:#pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    print(\"data shape:\", sub_train.shape[0])\n    \n    #feature selection\n    rfe = RandomForestClassifier()\n    rfe = RFECV(estimator=rfe, step=1, cv=2,scoring='roc_auc')   #5-fold cross-validation\n    rfe = rfe.fit(sub_train[features], sub_train['artis_durumu'])\n    best_features = sub_train[features].columns[rfe.support_]\n    print('Optimal number of features :', rfe.n_features_)\n    print('Best features :',best_features )\n    #best_features = features\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[best_features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   #learning_rate = 0.005,\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 300,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train,\n                 eval_metric = \"auc\",\n                 eval_set=[(X_train,y_train),(X_test,y_test)],\n                 callbacks=[early_stopping(stopping_rounds=50, first_metric_only=True),\n                            log_evaluation(period=10)])\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train,y_train)\n    \n    prob1 = lgbm_fit.predict_proba(X_test)\n    prob2 = lr_fit.predict_proba(X_test)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    \n    c1,c2 = optimal_ensemble(prob1,prob2,y_test)\n    prob = c1*prob1+c2*prob2\n    \n    cutoff = get_optimal_cutoff2(pd.DataFrame(prob)[1], X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n    \n    test_preds = pd.concat([test_preds,\n                       pd.DataFrame({ \n                           'model_index': i,\n                           'actual':y_test,\n                           'preds' :y_test_pred})]\n                      , ignore_index=True)\n    \n    #train performance\n    prob1 = lgbm_fit.predict_proba(X_train)\n    prob2 = lr_fit.predict_proba(X_train)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    prob = c1*prob1+c2*prob2\n    \n    y_train_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    train_f1_cutoff = f1_score(y_train,y_train_pred)\n    print(\"model_index \" + str(i) + \" : train_f1_score= \" + str(train_f1_cutoff))\n\n\n    ## submission preds\n    prob1 = lgbm_fit.predict_proba(sub_test[best_features])\n    prob2 = lr_fit.predict_proba(sub_test[best_features]) \n    prob = c1*prob1+c2*prob2\n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:31:01.582859Z","iopub.execute_input":"2022-05-22T00:31:01.583174Z","iopub.status.idle":"2022-05-22T00:43:09.92908Z","shell.execute_reply.started":"2022-05-22T00:31:01.583134Z","shell.execute_reply":"2022-05-22T00:43:09.927338Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(test_preds.actual,test_preds.preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:36:44.917223Z","iopub.execute_input":"2022-05-21T22:36:44.917518Z","iopub.status.idle":"2022-05-21T22:36:44.927331Z","shell.execute_reply.started":"2022-05-21T22:36:44.917486Z","shell.execute_reply":"2022-05-21T22:36:44.926647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deeper level with Kapsam_tipi","metadata":{}},{"cell_type":"code","source":"modeling_codes = pd.DataFrame(train.groupby(['musteri_segmenti','sigorta_tip'])['policy_id'].count()).reset_index().reset_index()\n#modeling_codes.sort_values('sigorta_tip')\n\n# sigorta_tip = 1 -- keep musteri_segmenti\n# sigorta_tip = 4 -- do not detail on musteri_segmenti / use musteri_segmenti= 102\n# sigorta_tip = 6 --  exclude 105 and 101\n# sigorta_tip = 7 -- ok\n# sigorta_tip = 8 -- do not detail on musteri_segmenti/ combine them\nmodeling_codes.rename(columns = {'index':'model_index'}, inplace = True)\n\nmodeling_codes['model_index'] = np.NaN\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([1,7]), 'model_index']= range(1, 1+ len(modeling_codes[modeling_codes['sigorta_tip'].isin([1,7])]))\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([4]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([8]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[(modeling_codes['sigorta_tip'].isin([6])) & (modeling_codes['musteri_segmenti'].isin([101,105])), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['model_index'].isnull() == True, 'model_index'] = range(16,20)\nmodeling_codes.loc[modeling_codes['model_index'] == 15,'model_index'] = 16\nmodeling_codes.loc[modeling_codes['model_index'] == 2,'model_index'] = 4\n##\nmodeling_codes2 = pd.DataFrame(train.groupby(['sigorta_tip','musteri_segmenti','kapsam_tipi'])['policy_id'].count()).reset_index().reset_index()\nmodeling_codes2 = modeling_codes2.sort_values('policy_id', ascending = False)\nmodeling_codes2 = modeling_codes2.merge(modeling_codes[['sigorta_tip','musteri_segmenti','model_index']], on = ['sigorta_tip','musteri_segmenti'])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:04:52.075729Z","iopub.execute_input":"2022-05-22T00:04:52.076016Z","iopub.status.idle":"2022-05-22T00:04:52.229628Z","shell.execute_reply.started":"2022-05-22T00:04:52.075983Z","shell.execute_reply":"2022-05-22T00:04:52.228772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION215'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION056'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION001'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION312'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION247'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION121'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION195'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION251'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION243'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION202'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION238'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['kapsam_tipi'] == 'PENSION245'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\n\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 9) & (modeling_codes2['kapsam_tipi'] == 'PENSION247'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 9) & (modeling_codes2['kapsam_tipi'] == 'PENSION251'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 9) & (modeling_codes2['kapsam_tipi'] == 'PENSION215'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 9) & (modeling_codes2['kapsam_tipi'] == 'PENSION312'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 9) & (modeling_codes2['kapsam_tipi'] == 'PENSION195'), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 9) & (modeling_codes2['kapsam_tipi'] == 'PENSION243'), 'model_index'] = 1 + max(modeling_codes2['model_index'])","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:04:54.368009Z","iopub.execute_input":"2022-05-22T00:04:54.368337Z","iopub.status.idle":"2022-05-22T00:04:54.420808Z","shell.execute_reply.started":"2022-05-22T00:04:54.368304Z","shell.execute_reply":"2022-05-22T00:04:54.4199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(columns = ['model_index'],inplace = True)\ntest.drop(columns = ['model_index'],inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:19:31.227212Z","iopub.status.idle":"2022-05-22T00:19:31.229073Z","shell.execute_reply.started":"2022-05-22T00:19:31.228608Z","shell.execute_reply":"2022-05-22T00:19:31.228655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(modeling_codes2[['sigorta_tip','musteri_segmenti','kapsam_tipi','model_index']], on = ['sigorta_tip','musteri_segmenti','kapsam_tipi'], how = 'left')\ntest = test.merge(modeling_codes2[['sigorta_tip','musteri_segmenti','kapsam_tipi','model_index']], on = ['sigorta_tip','musteri_segmenti','kapsam_tipi'], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:05:00.238242Z","iopub.execute_input":"2022-05-22T00:05:00.238538Z","iopub.status.idle":"2022-05-22T00:05:06.086008Z","shell.execute_reply.started":"2022-05-22T00:05:00.238502Z","shell.execute_reply":"2022-05-22T00:05:06.084891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['model_index'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:05:06.088435Z","iopub.execute_input":"2022-05-22T00:05:06.08881Z","iopub.status.idle":"2022-05-22T00:05:06.106866Z","shell.execute_reply.started":"2022-05-22T00:05:06.088761Z","shell.execute_reply":"2022-05-22T00:05:06.106159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## best config\n# lgbm & rf & opt ensemble.. new features .. \ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 200,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train,eval_metric = \"auc\",\n                 eval_set=[(X_train,y_train),(X_test,y_test)],\n                 callbacks=[early_stopping(stopping_rounds=50, first_metric_only=True),\n                            log_evaluation(period=10)])\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train,y_train)\n    \n    prob1 = lgbm_fit.predict_proba(X_test)\n    prob2 = lr_fit.predict_proba(X_test)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    \n    c1,c2 = optimal_ensemble(prob1,prob2,y_test)\n    prob = c1*prob1+c2*prob2\n    \n    cutoff = get_optimal_cutoff2(pd.DataFrame(prob)[1], X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n    \n    test_preds = pd.concat([test_preds,\n                       pd.DataFrame({ \n                           'model_index': i,\n                           'actual':y_test,\n                           'preds' :y_test_pred})]\n                      , ignore_index=True)\n    \n    #train performance\n    prob1 = lgbm_fit.predict_proba(X_train)\n    prob2 = lr_fit.predict_proba(X_train)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    prob = c1*prob1+c2*prob2\n    \n    y_train_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    train_f1_cutoff = f1_score(y_train,y_train_pred)\n    print(\"model_index \" + str(i) + \" : train_f1_score= \" + str(train_f1_cutoff))\n\n\n    ## submission preds\n    prob1 = lgbm_fit.predict_proba(sub_test[features])\n    prob2 = lr_fit.predict_proba(sub_test[features]) \n    prob = c1*prob1+c2*prob2\n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:05:18.508629Z","iopub.execute_input":"2022-05-22T00:05:18.508938Z","iopub.status.idle":"2022-05-22T00:14:52.047273Z","shell.execute_reply.started":"2022-05-22T00:05:18.508905Z","shell.execute_reply":"2022-05-22T00:14:52.046288Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(test_preds.actual,test_preds.preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:14:52.404619Z","iopub.execute_input":"2022-05-22T00:14:52.404918Z","iopub.status.idle":"2022-05-22T00:14:52.466793Z","shell.execute_reply.started":"2022-05-22T00:14:52.404884Z","shell.execute_reply":"2022-05-22T00:14:52.465982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deeper level with office_id","metadata":{}},{"cell_type":"code","source":"modeling_codes = pd.DataFrame(train.groupby(['musteri_segmenti','sigorta_tip'])['policy_id'].count()).reset_index().reset_index()\n#modeling_codes.sort_values('sigorta_tip')\n\n# sigorta_tip = 1 -- keep musteri_segmenti\n# sigorta_tip = 4 -- do not detail on musteri_segmenti / use musteri_segmenti= 102\n# sigorta_tip = 6 --  exclude 105 and 101\n# sigorta_tip = 7 -- ok\n# sigorta_tip = 8 -- do not detail on musteri_segmenti/ combine them\nmodeling_codes.rename(columns = {'index':'model_index'}, inplace = True)\n\nmodeling_codes['model_index'] = np.NaN\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([1,7]), 'model_index']= range(1, 1+ len(modeling_codes[modeling_codes['sigorta_tip'].isin([1,7])]))\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([4]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['sigorta_tip'].isin([8]), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[(modeling_codes['sigorta_tip'].isin([6])) & (modeling_codes['musteri_segmenti'].isin([101,105])), 'model_index'] = 1 + modeling_codes['model_index'].max()\nmodeling_codes.loc[modeling_codes['model_index'].isnull() == True, 'model_index'] = range(16,20)\nmodeling_codes.loc[modeling_codes['model_index'] == 15,'model_index'] = 16\nmodeling_codes.loc[modeling_codes['model_index'] == 2,'model_index'] = 4\n##\nmodeling_codes2 = pd.DataFrame(train.groupby(['sigorta_tip','musteri_segmenti','office_id'])['policy_id'].count()).reset_index().reset_index()\nmodeling_codes2 = modeling_codes2.sort_values('policy_id', ascending = False)\n\nmodeling_codes2 = modeling_codes2.merge(modeling_codes[['sigorta_tip','musteri_segmenti','model_index']], on = ['sigorta_tip','musteri_segmenti'])\n#modeling_codes2.loc[modeling_codes2['policy_id'] > 3000,'model_index']= range()\n\nmodeling_codes2[(modeling_codes2['policy_id'] > 2000)]\n\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 9) & (modeling_codes2['office_id'] == 10006), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['office_id'] == 10006), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['office_id'] == 10017), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 7) & (modeling_codes2['office_id'] == 10006), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 6) & (modeling_codes2['office_id'] == 10006), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 4) & (modeling_codes2['office_id'] == 10006), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 3) & (modeling_codes2['office_id'] == 10006), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 5) & (modeling_codes2['office_id'] == 10006), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['office_id'] == 10091), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['office_id'] == 10029), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['office_id'] == 10117), 'model_index'] = 1 + max(modeling_codes2['model_index'])\nmodeling_codes2.loc[(modeling_codes2['model_index'] == 11) & (modeling_codes2['office_id'] == 10023), 'model_index'] = 1 + max(modeling_codes2['model_index'])","metadata":{"execution":{"iopub.status.busy":"2022-05-21T23:40:23.300082Z","iopub.execute_input":"2022-05-21T23:40:23.300417Z","iopub.status.idle":"2022-05-21T23:40:23.361092Z","shell.execute_reply.started":"2022-05-21T23:40:23.300385Z","shell.execute_reply":"2022-05-21T23:40:23.360292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train.drop(columns = ['model_index_x','model_index_y'],inplace = True)\n#test.drop(columns = ['model_index_x','model_index_y'],inplace = True)\n\ntrain.drop(columns = ['model_index'],inplace = True)\ntest.drop(columns = ['model_index'],inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T00:03:31.162386Z","iopub.execute_input":"2022-05-22T00:03:31.163232Z","iopub.status.idle":"2022-05-22T00:03:32.500712Z","shell.execute_reply.started":"2022-05-22T00:03:31.16318Z","shell.execute_reply":"2022-05-22T00:03:32.499571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.merge(modeling_codes2[['sigorta_tip','musteri_segmenti','office_id','model_index']], on = ['sigorta_tip','musteri_segmenti','office_id'], how = 'left')\ntest = test.merge(modeling_codes2[['sigorta_tip','musteri_segmenti','office_id','model_index']], on = ['sigorta_tip','musteri_segmenti','office_id'], how = 'left')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T23:41:24.841434Z","iopub.execute_input":"2022-05-21T23:41:24.842321Z","iopub.status.idle":"2022-05-21T23:41:31.214832Z","shell.execute_reply.started":"2022-05-21T23:41:24.842274Z","shell.execute_reply":"2022-05-21T23:41:31.213885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['model_index'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T23:41:32.964545Z","iopub.execute_input":"2022-05-21T23:41:32.964822Z","iopub.status.idle":"2022-05-21T23:41:32.982584Z","shell.execute_reply.started":"2022-05-21T23:41:32.96479Z","shell.execute_reply":"2022-05-21T23:41:32.981375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## best config\n# lgbm & rf & opt ensemble.. new features .. \ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 100,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train,eval_metric = \"auc\",\n                 eval_set=[(X_train,y_train),(X_test,y_test)],\n                 callbacks=[early_stopping(stopping_rounds=30, first_metric_only=True),\n                            log_evaluation(period=10)])\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train,y_train)\n    \n    prob1 = lgbm_fit.predict_proba(X_test)\n    prob2 = lr_fit.predict_proba(X_test)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    \n    c1,c2 = optimal_ensemble(prob1,prob2,y_test)\n    prob = c1*prob1+c2*prob2\n    \n    cutoff = get_optimal_cutoff2(pd.DataFrame(prob)[1], X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n    \n    test_preds = pd.concat([test_preds,\n                       pd.DataFrame({ \n                           'model_index': i,\n                           'actual':y_test,\n                           'preds' :y_test_pred})]\n                      , ignore_index=True)\n    \n    #train performance\n    prob1 = lgbm_fit.predict_proba(X_train)\n    prob2 = lr_fit.predict_proba(X_train)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    prob = c1*prob1+c2*prob2\n    \n    y_train_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    train_f1_cutoff = f1_score(y_train,y_train_pred)\n    print(\"model_index \" + str(i) + \" : train_f1_score= \" + str(train_f1_cutoff))\n\n\n    ## submission preds\n    prob1 = lgbm_fit.predict_proba(sub_test[features])\n    prob2 = lr_fit.predict_proba(sub_test[features]) \n    prob = c1*prob1+c2*prob2\n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-21T23:43:40.424582Z","iopub.execute_input":"2022-05-21T23:43:40.42489Z","iopub.status.idle":"2022-05-21T23:54:08.884075Z","shell.execute_reply.started":"2022-05-21T23:43:40.424857Z","shell.execute_reply":"2022-05-21T23:54:08.883164Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = lr_fit.feature_importances_\n#\n# Sort the feature importance in descending order\n#\nsorted_indices = np.argsort(importances)[::-1]\n\nfig = plt.gcf()\nfig.set_size_inches(18, 8)\n\nplt.title('Feature Importance')\nplt.bar(range(X_train.shape[1]), importances[sorted_indices], align='center')\nplt.xticks(range(X_train.shape[1]), X_train.columns[sorted_indices], rotation=90)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-20T15:42:25.999317Z","iopub.execute_input":"2022-05-20T15:42:25.999696Z","iopub.status.idle":"2022-05-20T15:42:29.664379Z","shell.execute_reply.started":"2022-05-20T15:42:25.999662Z","shell.execute_reply":"2022-05-20T15:42:29.663295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(test_preds.actual,test_preds.preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T23:56:37.039791Z","iopub.execute_input":"2022-05-21T23:56:37.04008Z","iopub.status.idle":"2022-05-21T23:56:37.101655Z","shell.execute_reply.started":"2022-05-21T23:56:37.040047Z","shell.execute_reply":"2022-05-21T23:56:37.101018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling with SMOTE Sampling","metadata":{}},{"cell_type":"code","source":"# lgbm & rf & opt ensemble with soft SMOTE\ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n    X_test_scaled = scaler.transform(X_test)\n    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n    \n    ## SMOTE \n    over = SMOTE(sampling_strategy=0.2, k_neighbors=10,random_state = 0)\n    X_train_over, y_train_over = over.fit_resample(X_train_scaled, y_train.ravel())\n\n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 100,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train_over,y_train_over)\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train_over,y_train_over)\n    \n    prob1 = lgbm_fit.predict_proba(X_test_scaled)\n    prob2 = lr_fit.predict_proba(X_test_scaled)\n    \n    prob1 = pd.DataFrame(prob1)[1]\n    prob2 = pd.DataFrame(prob2)[1]\n    \n    c1,c2 = optimal_ensemble(prob1,prob2,y_test)\n    prob = c1*prob1+c2*prob2\n    \n    cutoff = get_optimal_cutoff2(pd.DataFrame(prob)[1], X_test_scaled, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n\n    test_preds = pd.concat([test_preds,\n                           pd.DataFrame({'actual':y_test,\n                                        'preds' :y_test_pred})]\n                          , ignore_index=True)\n    \n    sub_test_scaled = scaler.transform(sub_test[features])\n    prob1 = lgbm_fit.predict_proba(sub_test_scaled)\n    prob2 = lr_fit.predict_proba(sub_test_scaled) \n    prob = c1*prob1+c2*prob2\n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-16T23:42:10.451106Z","iopub.execute_input":"2022-05-16T23:42:10.451848Z","iopub.status.idle":"2022-05-16T23:51:11.917045Z","shell.execute_reply.started":"2022-05-16T23:42:10.451811Z","shell.execute_reply":"2022-05-16T23:51:11.91548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble of two models with mean probs","metadata":{}},{"cell_type":"code","source":"# lgbm & rf\ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        stratify = sub_train['artis_durumu'],\n                                                        random_state=0)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 100,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train)\n    \n    lr_fit = RandomForestClassifier(random_state=SEED,\n                                   n_estimators = 400) #class_weight\n    lr_fit.fit(X_train,y_train)\n    \n    prob1 = lgbm_fit.predict_proba(X_test)\n    prob2 = lr_fit.predict_proba(X_test)\n    prob = np.mean( np.array([ prob1, prob2 ]), axis=0 )\n    \n    cutoff = get_optimal_cutoff2(prob, X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n\n    test_preds = pd.concat([test_preds,\n                           pd.DataFrame({'actual':y_test,\n                                        'preds' :y_test_pred})]\n                          , ignore_index=True)\n    \n    prob1 = lgbm_fit.predict_proba(sub_test[features])\n    prob2 = lr_fit.predict_proba(sub_test[features])\n    prob = np.mean( np.array([ prob1, prob2 ]), axis=0 )    \n    \n    subm_pred = np.where(pd.DataFrame(prob)[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-16T22:20:03.650603Z","iopub.execute_input":"2022-05-16T22:20:03.650967Z","iopub.status.idle":"2022-05-16T22:28:19.287774Z","shell.execute_reply.started":"2022-05-16T22:20:03.650928Z","shell.execute_reply":"2022-05-16T22:28:19.286556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Only LGBM ","metadata":{}},{"cell_type":"code","source":"## only LGBM\ntest['pred'] = np.NaN\ntest_preds = pd.DataFrame()\n\nfor i in pd.unique(train.model_index):\n    print(i)\n    sub_train = train[(train['model_index'] == i)]\n    sub_test  = test[(test['model_index'] == i)]\n    \n    X_train, X_test, y_train, y_test = train_test_split(sub_train[features],\n                                                        sub_train['artis_durumu'],\n                                                        test_size=0.2,\n                                                        random_state=0)\n    \n    lgbm_fit = lgbm.LGBMClassifier(boosting_type='gbdt', \n                                   objective='binary', \n                                   metric='f1_score',\n                                   feature_fraction = 0.4,\n                                   bagging_fraction = 0.6,\n                                   n_estimators = 100,\n                                   max_depth = 3\n                                  )\n    lgbm_fit.fit(X_train,y_train)\n    \n    cutoff = get_optimal_cutoff(lgbm_fit, X_test, y_test)\n    y_test_pred= np.where(pd.DataFrame(lgbm_fit.predict_proba(X_test))[1]<=cutoff, 0, 1)\n    test_f1_cutoff = f1_score(y_test,y_test_pred)\n    print(\"model_index \" + str(i) + \" : test_f1_score= \" + str(test_f1_cutoff))\n\n    test_preds = pd.concat([test_preds,\n                           pd.DataFrame({'actual':y_test,\n                                        'preds' :y_test_pred})]\n                          , ignore_index=True)\n    subm_pred = np.where(pd.DataFrame(lgbm_fit.predict_proba(sub_test[features]))[1]<=cutoff, 0, 1)\n    test.loc[(test['model_index'] == i), 'pred'] = subm_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-16T16:00:07.081654Z","iopub.execute_input":"2022-05-16T16:00:07.082088Z","iopub.status.idle":"2022-05-16T16:00:21.624237Z","shell.execute_reply.started":"2022-05-16T16:00:07.082059Z","shell.execute_reply":"2022-05-16T16:00:21.623214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_score(test_preds.actual, test_preds.preds)","metadata":{"execution":{"iopub.status.busy":"2022-05-16T16:02:07.139013Z","iopub.execute_input":"2022-05-16T16:02:07.139615Z","iopub.status.idle":"2022-05-16T16:02:07.210869Z","shell.execute_reply.started":"2022-05-16T16:02:07.139575Z","shell.execute_reply":"2022-05-16T16:02:07.210011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions for Submission","metadata":{}},{"cell_type":"code","source":"sample = sample.merge(test[['policy_id','pred']], left_on = 'POLICY_ID',right_on = 'policy_id', how = 'left')\nsample.drop(columns = ['ARTIS_DURUMU','policy_id'],inplace = True)\nsample.columns = ['POLICY_ID','ARTIS_DURUMU']\nsample['ARTIS_DURUMU'] = np.where(sample['ARTIS_DURUMU']== 0.0, 0, 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:47:28.797803Z","iopub.execute_input":"2022-05-22T14:47:28.798108Z","iopub.status.idle":"2022-05-22T14:47:28.873748Z","shell.execute_reply.started":"2022-05-22T14:47:28.798076Z","shell.execute_reply":"2022-05-22T14:47:28.872738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample['ARTIS_DURUMU'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:49:10.654272Z","iopub.execute_input":"2022-05-22T14:49:10.654593Z","iopub.status.idle":"2022-05-22T14:49:10.661229Z","shell.execute_reply.started":"2022-05-22T14:49:10.654563Z","shell.execute_reply":"2022-05-22T14:49:10.660447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T14:49:16.395299Z","iopub.execute_input":"2022-05-22T14:49:16.395623Z","iopub.status.idle":"2022-05-22T14:49:16.640261Z","shell.execute_reply.started":"2022-05-22T14:49:16.395591Z","shell.execute_reply":"2022-05-22T14:49:16.639343Z"},"trusted":true},"execution_count":null,"outputs":[]}]}